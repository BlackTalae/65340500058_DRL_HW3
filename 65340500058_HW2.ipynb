{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2: Model-free RL**\n",
    "#### **Created by 65340500058 Anuwit Intet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives:**\n",
    "\n",
    "- Understand how a reinforcement learning agent learns (i.e., evaluates and improves its policy) in an environment where the true dynamic model is unknown.\n",
    "\n",
    "- Gain insight into different reinforcement learning algorithms, including Monte Carlo methods, the SARSA algorithm, Q-learning, and Double Q-learning. Analyze their strengths and weaknesses.\n",
    "\n",
    "- Explore approaches to implementing reinforcement learning in real-world scenarios where the state and action spaces are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from RL_Algorithm.RL_base import ControlType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 1: Setting up Cart-Pole Agent**</font>\n",
    "\n",
    "For the first part of this homework, you will implement a Cart-Pole agent from scratch, i.e., you must implement the constructor and core functions of the RL Base Class, as well as the algorithms in the Algorithm folder. The core components should include, but are not limited to:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RL Base class\n",
    "- This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - Control type: Enumeration of RL algorithms used for decision-making (i.e. Monte Carlo, Temporal Difference, Q-learning, or Double Q-learning).\n",
    "\n",
    "        - Number of actions: The total number of discrete actions available to the agent.\n",
    "\n",
    "        - Action range: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "        - Discretize state weight: Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "        - Learning rate: Determines how quickly the model updates based on new information.\n",
    "\n",
    "        - Initial epsilon: The starting probability of taking a random action in an ε-greedy policy.\n",
    "\n",
    "        - Epsilon decay rate: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "        - Final epsilon: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "        - Discount factor: A coefficient (γ) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - get_discretize_action(): Returns a discrete action based on the current policy.\n",
    "\n",
    "        - mapping_action(): Converts a discrete action back into a continuous action within the defined action range.\n",
    "\n",
    "        - discretize_state(): Discretizes and scales the state based on observation weights.\n",
    "\n",
    "        - decay_epsilon(): Decreases epsilon over time and returns the updated value.\n",
    "\n",
    "Additional details about these functions are provided in the class file.\n",
    "\n",
    "- Note: The RL Base Class also include two additional functions:\n",
    "\n",
    "    - save_q_value() which save model function from Q(s,a) as defaultdict.\n",
    "\n",
    "    - load_q_value() which load model function from Q(s,a) as defaultdict.\n",
    "\n",
    "You may also implement additional functions for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**discretize_state()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(self, obs: dict): # write in own task\n",
    "    \"\"\" \n",
    "    Discretize the observation state.\n",
    "\n",
    "    Args:\n",
    "        obs (dict): Observation dictionary containing policy states.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]: Discretized state.\n",
    "    \"\"\"\n",
    "\n",
    "    # ========= put your code here ==========\n",
    "    \n",
    "    # define number of value\n",
    "    pose_cart_bin = self.discretize_state_weight[0]\n",
    "    pose_pole_bin = self.discretize_state_weight[1]\n",
    "    vel_cart_bin = self.discretize_state_weight[2]\n",
    "    vel_pole_bin = self.discretize_state_weight[3]\n",
    "\n",
    "    # Clipping value\n",
    "    pose_cart_bound = 3\n",
    "    pose_pole_bound = float(np.deg2rad(24.0))\n",
    "    vel_cart_bound = 10\n",
    "    vel_pole_bound = 10\n",
    "    \n",
    "    # get observation term from continuos space\n",
    "    pose_cart_raw, pose_pole_raw , vel_cart_raw , vel_pole_raw = obs['policy'][0, 0] , obs['policy'][0, 1] , obs['policy'][0, 2] , obs['policy'][0, 3]\n",
    "\n",
    "    print(f\"Raw Values: pose_cart={pose_cart_raw}, pose_pole={pose_pole_raw}, vel_cart={vel_cart_raw}, vel_pole={vel_pole_raw}\")\n",
    "\n",
    "    pose_cart_clip = torch.clip(pose_cart_raw , -pose_cart_bound ,pose_cart_bound)\n",
    "    pose_pole_clip = torch.clip(pose_pole_raw , -pose_pole_bound ,pose_pole_bound)\n",
    "    vel_cart_clip = torch.clip(vel_cart_raw , -vel_cart_bound ,vel_cart_bound)\n",
    "    vel_pole_clip = torch.clip(vel_pole_raw , -vel_pole_bound ,vel_pole_bound)\n",
    "\n",
    "    print(f\"Clipped Values: pose_cart={pose_cart_clip}, pose_pole={pose_pole_clip}, vel_cart={vel_cart_clip}, vel_pole={vel_pole_clip}\")\n",
    "\n",
    "    device = pose_cart_clip.device\n",
    "\n",
    "    # create grid for discretization\n",
    "    pose_cart_grid = torch.linspace(-pose_cart_bound , pose_cart_bound , pose_cart_bin , device=device)\n",
    "    pose_pole_grid = torch.linspace(-pose_pole_bound , pose_pole_bound , pose_pole_bin , device=device)\n",
    "    vel_cart_grid = torch.linspace(-vel_cart_bound , vel_cart_bound , vel_cart_bin , device=device)\n",
    "    vel_pole_grid = torch.linspace(-vel_pole_bound , vel_pole_bound , vel_pole_bin , device=device)\n",
    "\n",
    "    print(f\"Pose Cart Grid: {pose_cart_grid}\")\n",
    "    print(f\"Pose Pole Grid: {pose_pole_grid}\")\n",
    "    print(f\"Vel Cart Grid: {vel_cart_grid}\")\n",
    "    print(f\"Vel Pole Grid: {vel_pole_grid}\")\n",
    "    \n",
    "    # Discretization\n",
    "    pose_cart_dig = torch.bucketize(pose_cart_clip,pose_cart_grid)\n",
    "    pose_pole_dig = torch.bucketize(pose_pole_clip,pose_pole_grid)\n",
    "    vel_cart_dig = torch.bucketize(vel_cart_clip,vel_cart_grid)\n",
    "    vel_pose_dig = torch.bucketize(vel_pole_clip,vel_pole_grid)\n",
    "\n",
    "    print(f\"Discretized Values: pose_cart={pose_cart_dig}, pose_pole={pose_pole_dig}, vel_cart={vel_cart_dig}, vel_pole={vel_pole_dig}\")\n",
    "\n",
    "    return (int(pose_cart_dig), int(pose_pole_dig), int(vel_cart_dig),  int(vel_pose_dig))\n",
    "\n",
    "    # ======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say the code works like this:\n",
    "- pose_cart_raw=2.5, pose_pole_raw=0.2617993950843811, vel_cart_raw=5.0, vel_pole_raw=-7.0\n",
    "\n",
    "- self.discretize_state_weight = [10, 10, 10, 10]\n",
    "\n",
    "==========================================================================================================================\n",
    "\n",
    "`torch.clip` is the function that clips value to the proper range.\n",
    "\n",
    "```python\n",
    "Ex. \n",
    "Clipped Values: pose_cart=2.5, pose_pole=0.2617993950843811, vel_cart=5.0, vel_pole=-7.0\n",
    "```\n",
    "\n",
    "The value is not correct because it is within the specified range.\n",
    "\n",
    "==========================================================================================================================\n",
    "\n",
    "`torch.linspace` Create a break point for discretization by: torch.linspace(start, end, steps, device=device) → Create a range of values ​​from start to end, divided into a number of steps.\n",
    "\n",
    "```python\n",
    "Ex.\n",
    "Pose Cart Grid: tensor([-3.0000, -2.3333, -1.6667, -1.0000, -0.3333,  0.3333,  1.0000,  1.6667,  2.3333,  3.0000])\n",
    "Pose Pole Grid: tensor([-0.4189, -0.3255, -0.2321, -0.1387, -0.0454,  0.0480,  0.1414,  0.2348,  0.3282,  0.4216])\n",
    "Vel Cart Grid: tensor([-10.0000,  -7.7778,  -5.5556,  -3.3333,  -1.1111,   1.1111,   3.3333,   5.5556,   7.7778,  10.0000])\n",
    "Vel Pole Grid: tensor([-10.0000,  -7.7778,  -5.5556,  -3.3333,  -1.1111,   1.1111,   3.3333,   5.5556,   7.7778,  10.0000])\n",
    "```\n",
    "\n",
    "Each variable is divided into 10 ranges (bin=10).\n",
    "\n",
    "==========================================================================================================================\n",
    "\n",
    "`torch.bucketize` is used to specify which bin range x falls in, which will give the index of that bin.\n",
    "\n",
    "```python\n",
    "Ex. \n",
    "Discretized Values: pose_cart=9, pose_pole=8, vel_cart=7, vel_pole=2\n",
    "```\n",
    "\n",
    "pose_cart=9 → is in bin 9, pose_pole=8 → is in bin 8, vel_cart=7 → is in bin 7, vel_pole=2 → is in bin 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**get_discretize_action()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discretize_action(self, obs_dis): # -> int (write in own task)\n",
    "    \"\"\"\n",
    "    Select an action using an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        obs_dis (tuple): Discretized observation.\n",
    "\n",
    "    Returns:\n",
    "        int: Chosen discrete action index.\n",
    "    \"\"\"\n",
    "    # ========= put your code here =========#\n",
    "\n",
    "    if np.random.rand() < self.epsilon:\n",
    "        # Exploration\n",
    "        return np.random.randint(self.num_of_action)\n",
    "    else:\n",
    "        # Exploitation\n",
    "        if self.control_type == ControlType.DOUBLE_Q_LEARNING:\n",
    "            return int(np.argmax(self.qa_values[obs_dis] + self.qb_values[obs_dis]))\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs_dis]))\n",
    "    \n",
    "    # ======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is Action Selection divided into 2 sections:\n",
    "\n",
    "- Exploration: If the value of np.random.rand() (a random number between 0 and 1) is less than self.epsilon, the function chooses a random action (use np.random.randint(self.num_of_action) to choose a random action from a total of actions).\n",
    "\n",
    "- Exploitation: If the value of np.random.rand() is not less than self.epsilon, the function selects the action with the highest Q-value from the available actions:\n",
    "\n",
    "    - If Double Q-Learning is used, the function calculates the Q-value by adding the qa_values ​​and qb_values ​​for the obs_dis state and selecting the action with the highest value using np.argmax.\n",
    "\n",
    "    - If Double Q-Learning is not used, the function selects the action with the highest Q-value from the q_values ​​for the obs_dis state.\n",
    "\n",
    "Let's say:\n",
    "```python\n",
    "    self.num_of_action = 9 \n",
    "    self.epsilon = 0.1 \n",
    "    obs_dis = (2, 4, 1, 1) \n",
    "    self.qa_values = {(2, 4, 1, 1): [0.5, 1.0, 0.7, 0.8, 1.5, 0.4, 0.3, 0.9, 1.2]}\n",
    "    self.qb_values = {(2, 4, 1, 1): [0.4, 1.1, 0.6, 0.7, 1.4, 0.3, 0.5, 0.8, 1.3]}\n",
    "    self.q_values = {(2, 4, 1, 1): [0.6, 1.0, 0.8, 0.9, 1.6, 0.5, 0.4, 1.1, 1.4]}\n",
    "```\n",
    "\n",
    "- Exploration action selection: If np.random.rand() is less than self.epsilon (0.1), the function selects a random action, such as action = np.random.randint(9) (i.e. selects the action at index 2).\n",
    "\n",
    "- Exploitation action selection: If np.random.rand() is greater than self.epsilon (0.1), the function selects the action with the highest value from the Q-values:\n",
    "\n",
    "    - In case of Double Q-Learning: Compute the sum of qa_values ​​and qb_values ​​for the state (2, 4, 1, 1) and select the highest value\n",
    "\n",
    "    - In case of not using Double Q-Learning: Select the action with the highest Q-value from q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**mapping_action()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_action(self, action):\n",
    "    \"\"\"\n",
    "    Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "    Args:\n",
    "        action (int): Discrete action in range [0, n]\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled action tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Linear scaling to map discrete action to continuous range\n",
    "    action_cont = self.action_range[0] + (action / (self.num_of_action - 1)) * (self.action_range[1] - self.action_range[0])\n",
    "\n",
    "    return torch.tensor(action_cont, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converts action values ​​in the range [0, n] to continuous values ​​in the range [action_min, action_max] using linear scaling.\n",
    "\n",
    "    - action_range[0]: Minimum value of the range\n",
    "\n",
    "    - action_range[1]: Maximum value of the range\n",
    "\n",
    "    - The division (self.num_of_action - 1) is used to convert all actions in the range 0 to n to fit within the continuous range.\n",
    "\n",
    "- Suppose self.action_range = [-12.0, 12.0] and self.num_of_action = 9:\n",
    "\n",
    "    - if action = 0 (first action): action_cont = -12.0 + (0 / 8) * (12.0 - (-12.0)) = -12.0\n",
    "\n",
    "    - if action = 4 (middle action): action_cont = -12.0 + (4 / 8) * (12.0 - (-12.0)) = 0.0\n",
    "\n",
    "    - if action = 8 (last action): action_cont = -12.0 + (8 / 8) * (12.0 - (-12.0)) = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**decay_epsilon()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self): # write in own task\n",
    "    \"\"\"\n",
    "    Decay epsilon value to reduce exploration over time.\n",
    "    \"\"\"\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce epsilon gradually in each learning cycle by multiplying self.epsilon by self.epsilon_decay , we get an exponential decay curve\n",
    "\n",
    "- Prevent epsilon from falling below a certain minimum value (self.final_epsilon) by using max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "- Example self.epsilon_decay = 0.996\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Algorithm folder\n",
    "\n",
    "- This folder should include:\n",
    "\n",
    "    - Monte Carlo class\n",
    "\n",
    "    - SARSA class\n",
    "\n",
    "    - Q-Learning Class\n",
    "\n",
    "    - Double Q-Learning Class\n",
    "\n",
    "- Each class should inherit from the RL Base class and include:\n",
    "\n",
    "    - A constructor which initializes the same variables as the class it inherits from.\n",
    "\n",
    "    - Superclass Initialization (super().__init__()).\n",
    "\n",
    "    - An update() function that updates the agent’s learnable parameters and advances the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Monte Carlo class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Control uses environmental exploration and calculates the expected value of each `(s, a)` using the returns from the simulation (Episode-based Learning). This process is done:\n",
    "\n",
    "**1. Play the Episode**\n",
    "Compile the sequence of $(s_t, a_t, r_t, s_{t+1})$ until it is complete.\n",
    "\n",
    "**2. Back to calculate the return**\n",
    "Use the formula Return (G) to accumulate backward:\n",
    "\n",
    "$$\n",
    "G_t = r_t + \\gamma G_{t+1}\n",
    "$$\n",
    "\n",
    "- $G_t$ is the accumulated return from timestep \\( t \\) onwards\n",
    "- $\\gamma$ (discount factor) is used to reduce the future return\n",
    "\n",
    "**3. First-Visit MC**\n",
    "Only when the state-action `(s, a)` is first encountered in that episode:\n",
    "\n",
    "Adjust the Q-value according to the formula:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha (G - Q(s,a))\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, which is calculated as:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{N(s,a)}\n",
    "$$\n",
    "\n",
    "So if $N(s,a)$ is increased, the Q-value will be stabilized.\n",
    "\n",
    "**4.Policy Improvement**\n",
    "- **Choose action by greedy policy**:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a Q(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(\n",
    "    self,\n",
    "    state,  # state of the current time step\n",
    "    action: int,  # action taken\n",
    "    reward, # reward received\n",
    "    next_state,  # state after action is taken  \n",
    "    done: bool = False,  # indicates end of episode   \n",
    "):\n",
    "    \"\"\"\n",
    "    Update Q-values using Monte Carlo.\n",
    "\n",
    "    This method applies the Monte Carlo update rule to improve policy decisions by updating the Q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    state = self.discretize_state(state)  # Convert the state into a discrete representation\n",
    "\n",
    "    # Store transition in memory for later updates\n",
    "    self.episode_memory.append((state, action, reward))\n",
    "\n",
    "    # Perform updates only at the end of the episode\n",
    "    if done:\n",
    "        G = 0  # Initialize return (G) to zero\n",
    "        visited = set()  # Keep track of visited (state, action) pairs\n",
    "\n",
    "        # Iterate through the episode in reverse order (from the last time step to the first)\n",
    "        for t in reversed(range(len(self.episode_memory))):\n",
    "            s_t, a_t, r_t = self.episode_memory[t]  # Get state, action, and reward at timestep t\n",
    "            G = r_t + self.discount_factor * G  # Compute return G using the Bellman equation\n",
    "\n",
    "            # First-Visit Monte Carlo: update only if (s_t, a_t) is encountered for the first time\n",
    "            if (s_t, a_t) not in visited:\n",
    "                visited.add((s_t, a_t))  # Mark this (s, a) pair as visited\n",
    "\n",
    "                # Count the number of times this (state, action) pair has been visited\n",
    "                self.n_values[s_t][a_t] += 1  \n",
    "                \n",
    "                # Store the return for this (state, action) pair\n",
    "                self.returns[(s_t, a_t)].append(G)\n",
    "\n",
    "                # Compute learning rate (alpha) as the inverse of visit count (1 / N(s, a))\n",
    "                alpha = 1.0 / self.n_values[s_t][a_t]\n",
    "\n",
    "                # Monte Carlo Q-value update using incremental averaging\n",
    "                self.q_values[s_t][a_t] += alpha * (G - self.q_values[s_t][a_t])\n",
    "        \n",
    "        # Clear the episode memory after the update to prepare for the next episode\n",
    "        self.episode_memory = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**SARSA class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SARSA` is an **On-Policy Temporal Difference (TD) Learning** algorithm that uses Reinforcement Learning (RL) to learn an **action-value function** $Q(s,a)$ with the following rule updates:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "**SARSA Equation Parameters**\n",
    "\n",
    "- $s_t$ = current state \n",
    "- $a_t$ = action taken\n",
    "- $r_t$ = reward received \n",
    "- $s_{t+1}$ = next state\n",
    "- $a_{t+1}$ = next action chosen according to the current policy\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor\n",
    "\n",
    "**Key Properties of SARSA:**\n",
    "- **On-Policy**: Select the next action $a_{t+1}$ using **the same policy as currently learned** \n",
    "- **TD Learning**: Update the value of $Q(s,a)$ online, without waiting for the episode to end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(\n",
    "    self,\n",
    "    state,  # state of the current time step\n",
    "    action: int,  # action taken\n",
    "    reward: float,  # reward received\n",
    "    next_state,  # state after action is taken\n",
    "    next_action: int  # next action taken\n",
    "):\n",
    "    \"\"\"\n",
    "    Update Q-values using SARSA.\n",
    "\n",
    "    This method applies the SARSA update rule to improve policy decisions by updating the Q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert current and next states into discrete representations\n",
    "    current_state_discrete = self.discretize_state(state)\n",
    "    next_state_discrete = self.discretize_state(next_state)\n",
    "\n",
    "    # Retrieve the Q-value of the current (state, action) pair\n",
    "    current_q = self.q_values[tuple(current_state_discrete)][action]\n",
    "\n",
    "    # Retrieve the Q-value of the next (state, action) pair, which is chosen using the same policy\n",
    "    next_q = self.q_values[tuple(next_state_discrete)][next_action]\n",
    "\n",
    "    # Compute the target using the SARSA update rule: r + γ * Q(s', a')\n",
    "    target = reward + self.discount_factor * next_q\n",
    "\n",
    "    # Update Q-value using learning rate (α) and Temporal Difference (TD) error\n",
    "    self.q_values[tuple(current_state_discrete)][action] += self.learning_rate * (target - current_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Q-Learning Class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q-Learning` is an **Off-Policy Temporal Difference (TD) Learning** algorithm that uses Reinforcement Learning (RL) to learn an **optimal action-value function** $Q(s,a)$ with the following rule updates:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "**Q-Learning Equation Parameters:**\n",
    "\n",
    "- $s_t$ = current state\n",
    "- $a_t$ = action taken\n",
    "- $r_t$ = reward received\n",
    "- $s_{t+1}$ = next state\n",
    "- $\\max_{a} Q(s_{t+1}, a)$ = maximum Q-value for the next state across all possible actions)\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor\n",
    "\n",
    "**Key Properties of Q-Learning**\n",
    "- Off-Policy Learning: Uses the greedy action for updates, meaning the update does not depend on the current policy used for exploration. (Lack to explore diversity)\n",
    "\n",
    "- More Exploitative: Since it always picks the action with the highest future reward, it learns the optimal policy faster than SARSA. (Overestimation)\n",
    "\n",
    "- Less Stable in Noisy Environments: Since Q-learning always assumes the best action will be taken, it can be more sensitive to approximation errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(\n",
    "    self,\n",
    "    state,  # state of the current time step\n",
    "    action: int,  # action taken\n",
    "    reward: float,  # reward received\n",
    "    next_state,  # state after action is taken\n",
    "):\n",
    "    \"\"\"\n",
    "    Update Q-values using Q-Learning.\n",
    "\n",
    "    This method applies the Q-Learning update rule to improve policy decisions by updating the Q-table.\n",
    "    \"\"\"\n",
    "    state = self.discretize_state(state)\n",
    "    next_state = self.discretize_state(next_state)\n",
    "\n",
    "    # print(\"This is \", state)\n",
    "\n",
    "    # Get the Q-value for the current state and action\n",
    "    current_q_value = self.q_values[state][action]\n",
    "\n",
    "    # Get the maximum Q-value for the next state\n",
    "    max_future_q = np.max(self.q_values[next_state])\n",
    "\n",
    "    # Q-learning update rule\n",
    "    updated_q_value = current_q_value + self.learning_rate * (reward + self.discount_factor * max_future_q - current_q_value)\n",
    "\n",
    "    # Update Q-value in the Q-table\n",
    "    self.q_values[state][action] = updated_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Double Q-Learning Class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Double Q-Learning` is an improvement over **Q-Learning** to reduce the **Overestimation Bias** that occurs in the Q-value update in the original algorithm. The problem is that selecting the **best action (argmax)** and **evaluating the value of that action** use the same Q-table, which leads to **Overestimation Bias** in estimating the Q-value.\n",
    "\n",
    "**How to solve the problem with Double Q-Learning**\n",
    "\n",
    "Double Q-Learning uses **two Q-tables**: $Q_A$ and $Q_B$ to separate the **action selection** and **Q-value estimation** processes:\n",
    "\n",
    "$$\n",
    "Q_A(s_t, a_t) \\leftarrow Q_A(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q_B(s_{t+1}, \\arg\\max_a Q_A(s_{t+1}, a)) - Q_A(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q_B(s_t, a_t) \\leftarrow Q_B(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q_A(s_{t+1}, \\arg\\max_a Q_B(s_{t+1}, a)) - Q_B(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "**How ​​Double Q-Learning Works:**\n",
    "1. Randomly select the update to use $Q_A$ or $Q_B$ at each step.\n",
    "\n",
    "2. Select the best action from one Q-table (e.g. $\\arg\\max_a Q_A(s_{t+1}, a)$)\n",
    "\n",
    "3. Evaluate the action from another Q-table (e.g. $Q_B(s_{t+1}, \\arg\\max_a Q_A(s_{t+1}, a))$)\n",
    "\n",
    "**Key Properties of Double Q-Learning**\n",
    "- Reduce Overestimation Bias compared with Q-Learning.\n",
    "\n",
    "- Helps to make learning more stable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(\n",
    "    self,\n",
    "    #========= put your code here =========#\n",
    "    state,  # state of the current time step\n",
    "    action: int,  # action taken\n",
    "    reward: float,  # reward received\n",
    "    next_state,  # state after action is taken\n",
    "):\n",
    "    \"\"\"\n",
    "    Update Q-values using Double Q-Learning.\n",
    "\n",
    "    This method applies the Double Q-Learning update rule to improve policy decisions by updating the Q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    state = self.discretize_state(state)\n",
    "    next_state = self.discretize_state(next_state)\n",
    "    \n",
    "    if np.random.rand() < 0.5:\n",
    "        best_action = np.argmax(self.qa_values[next_state])\n",
    "        self.qa_values[state][action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * self.qb_values[next_state][best_action] - self.qa_values[state][action]\n",
    "        )\n",
    "    else:\n",
    "        best_action = np.argmax(self.qb_values[next_state])\n",
    "        self.qb_values[state][action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * self.qa_values[next_state][best_action] - self.qb_values[state][action]\n",
    "        )\n",
    "    #======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 2: Trainning & Playing to stabilize Cart-Pole Agent**</font>\n",
    "\n",
    "You need to implement the training loop in train script and main() in the play script (in the \"Can be modified\" area of both files). Additionally, you must collect data, analyze results, and save models for evaluating agent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training the Agent\n",
    "\n",
    "    - Stabilizing Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/RL_Algorithm/train.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/RL_Algorithm/train.py --task SwingUp-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "- Playing\n",
    "\n",
    "    - Stabilize Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/RL_Algorithm/play.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ``` \n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/RL_Algorithm/play.py --task SwingUp-Isaac-Cartpole-v0 \n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Train.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of each algorithm is quite similar, it has a little bit difference such as algorithm name, agent class etc. So I show the example of Monte Carlo Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases (WandB) for experiment tracking\n",
    "wandb.init(\n",
    "    project='DRL',  # Project name in WandB\n",
    "    name=\"MC\"       # Experiment name\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Hyperparameters Setup\n",
    "# ==========================\n",
    "\n",
    "num_of_action = 9  # Number of discrete actions the agent can take\n",
    "action_range = [-12.0, 12.0]  # Minimum and maximum action values\n",
    "discretize_state_weight = [5, 11, 3, 3]  # Number of bins for discretizing state space [pose_cart, pose_pole, vel_cart, vel_pole]\n",
    "learning_rate = 0.25  # Learning rate for the MC agent\n",
    "n_episodes = 5000  # Number of episodes for training\n",
    "start_epsilon = 1.0  # Initial epsilon (fully exploratory)\n",
    "epsilon_decay = 0.9996  # Decay rate for epsilon (reducing exploration over time)\n",
    "final_epsilon = 0.1  # Minimum epsilon (ensuring some level of exploration)\n",
    "discount = 0.5  # Discount factor for future rewards\n",
    "\n",
    "# Extract task name (e.g., 'Stabilize', 'SwingUp')\n",
    "task_name = str(args_cli.task).split('-')[0]\n",
    "\n",
    "# Algorithm name\n",
    "Algorithm_name = \"MC\"\n",
    "\n",
    "# Initialize Monte Carlo (MC) agent\n",
    "agent = MC(\n",
    "    num_of_action=num_of_action,\n",
    "    action_range=action_range,\n",
    "    discretize_state_weight=discretize_state_weight,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    discount_factor=discount\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Initialize Environment\n",
    "# ==========================\n",
    "obs, _ = env.reset()  # Reset environment and get initial observation\n",
    "timestep = 0  # Initialize timestep counter\n",
    "sum_reward = 0  # Track cumulative rewards over episodes\n",
    "sum_step = 0  # Track total steps taken over episodes\n",
    "\n",
    "# ==========================\n",
    "# Start Training Loop\n",
    "# ==========================\n",
    "while simulation_app.is_running():  # Run while the simulation is active\n",
    "\n",
    "    with torch.inference_mode():  # Disable gradient computation for inference\n",
    "\n",
    "        for episode in tqdm(range(n_episodes)):  # Iterate through training episodes\n",
    "            obs, _ = env.reset()  # Reset environment at the start of each episode\n",
    "            done = False  # Episode termination flag\n",
    "            cumulative_reward = 0  # Total reward per episode\n",
    "            temp_step = 0  # Count steps per episode\n",
    "\n",
    "            while not done:\n",
    "                # Select action using the agent's policy\n",
    "                action, action_idx = agent.get_action(obs)\n",
    "\n",
    "                # Reshape action into a 2D tensor (1 row, n columns)\n",
    "                action = action.view(1, -1)  \n",
    "\n",
    "                # Step the environment with the chosen action\n",
    "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # Convert reward and termination flags to scalar values\n",
    "                reward_value = torch.Tensor(reward).item()\n",
    "                terminated_value = torch.Tensor(terminated).item()\n",
    "\n",
    "                # Accumulate total reward for the episode\n",
    "                cumulative_reward += reward_value\n",
    "\n",
    "                # Determine if the episode is done\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Update the agent using the MC algorithm\n",
    "                agent.update(\n",
    "                    state=obs, \n",
    "                    action=action_idx, \n",
    "                    reward=reward_value, \n",
    "                    next_state=next_obs, \n",
    "                    done=done\n",
    "                )\n",
    "\n",
    "                # Update the observation for the next step\n",
    "                obs = next_obs  \n",
    "                temp_step += 1  # Increment step counter\n",
    "\n",
    "            # Update total reward and step count over episodes\n",
    "            sum_reward += cumulative_reward\n",
    "            sum_step += temp_step\n",
    "\n",
    "            # Log metrics to WandB\n",
    "            wandb.log({\n",
    "                'epsilon': agent.epsilon,  # Current exploration rate\n",
    "                'num_step': temp_step      # Steps taken in this episode\n",
    "            })\n",
    "            \n",
    "            # Every 100 episodes, log additional statistics\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'avg_reward' : sum_reward / 100.0,  # Average reward over last 100 episodes\n",
    "                    'avg_step': sum_step / 100.0,  # Average steps per episode\n",
    "                    'num_action': num_of_action,\n",
    "                    'discrete_weight_cart_pos': discretize_state_weight[0],\n",
    "                    'discrete_weight_pole_pos': discretize_state_weight[1],\n",
    "                    'discrete_weight_cart_vel': discretize_state_weight[2],\n",
    "                    'discrete_weight_pole_vel': discretize_state_weight[3],\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'epsilon_decay': epsilon_decay,\n",
    "                    'discount_factor': discount\n",
    "                })\n",
    "                \n",
    "                print(\"avg_score: \", sum_reward / 100.0)  # Print average reward\n",
    "                sum_reward = 0  # Reset cumulative reward counter\n",
    "                sum_step = 0  # Reset step counter\n",
    "\n",
    "                # Save Q-values for the trained MC agent\n",
    "                q_value_file = f\"{Algorithm_name}_{episode}_{num_of_action}_{action_range[1]}_{discretize_state_weight[0]}_{discretize_state_weight[1]}.json\"\n",
    "                full_path = os.path.join(f\"q_value/{task_name}\", Algorithm_name)\n",
    "                agent.save_q_value(full_path, q_value_file)\n",
    "\n",
    "            # Decay epsilon to gradually reduce exploration\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "    # If video recording is enabled, increment the timestep counter\n",
    "    if args_cli.video:\n",
    "        timestep += 1\n",
    "        # Stop the simulation after recording the specified number of frames\n",
    "        if timestep == args_cli.video_length:\n",
    "            break\n",
    "\n",
    "    print(\"!!! Training is complete !!!\")  # Indicate completion of training\n",
    "    break  # Exit training loop\n",
    "\n",
    "# Finalize WandB logging\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Play.py**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_action = 9 # edit this\n",
    "action_range = [-12, 12]\n",
    "discretize_state_weight = [5, 11, 3, 3] # edit this\n",
    "learning_rate = 0.25\n",
    "n_episodes = 5000\n",
    "start_epsilon = 0.0 # Edit this to zero\n",
    "epsilon_decay = 0.9996\n",
    "final_epsilon = 0.0 \n",
    "discount = 0.3 \n",
    "\n",
    "# Edit the agent\n",
    "agent = Q_Learning(\n",
    "    num_of_action=num_of_action,\n",
    "    action_range=action_range,\n",
    "    discretize_state_weight=discretize_state_weight,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    discount_factor=discount\n",
    ")\n",
    "\n",
    "task_name = str(args_cli.task).split('-')[0]  # Stabilize, SwingUp\n",
    "Algorithm_name = \"Q_Learning\"  # edit this\n",
    "episode = 99 # edit this\n",
    "q_value_file = f\"{Algorithm_name}_{episode}_{num_of_action}_{action_range[1]}_{discretize_state_weight[0]}_{discretize_state_weight[1]}.json\"\n",
    "full_path = os.path.join(f\"q_value/{task_name}\", Algorithm_name)\n",
    "agent.load_q_value(full_path, q_value_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 3: Evaluate Cart-Pole Agent performance**</font>\n",
    "\n",
    "You must evaluate the agent's performance in terms of learning efficiency (i.e., how well the agent learns to receive higher rewards) and deployment performance (i.e., how well the agent performs in the Cart-Pole problem). Analyze and visualize the results to determine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which algorithm performs best?\n",
    "\n",
    "- Why does it perform better than the others?\n",
    "\n",
    "- How do the resolutions of the action space and observation space affect the learning process? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightgreen\">**In terms of learning efficiency**</font> I will analyze using graph of average per 100 steps and analyze how each performs and why they perform like that including strength and weakness of each and summarize which is perform the best and why.\n",
    "\n",
    "<font color=\"lightgreen\">**In terms of deployment**</font> I will analyze using the pole position in the maximum time that each algorithm can stabilize (Use q_value at timestep 5000 in train period and run play period for 10 steps) and analyze in view of how pole oscillate and and summarize which is perform the best and why.\n",
    "\n",
    "The name of the graph in terms of deployment is `{Algorithm_name}_{num_of_action}_{action_range[1]}_{discretize_state_weight[0]}_{discretize_state_weight[1]}_{discretize_state_weight[2]}_{discretize_state_weight[3]}_{learning_rate}_{epsilon_decay}_{discount}`\n",
    "\n",
    "I select Monte Carlo to represent sampling and on-policy algorithm. SARSA to represent bootstraping and on-policy algorithm. And Q Learning to represent bootstraping and off-policy algorithm. I'm not select Double Q-learning to analyze when adjust hyperparameter because I have the assumption that Q-Learning and Double Q-Learning have the same representation(bootstraping and off-policy algorithm) and they have the same result when adjust hyperparameter (except discount factor because Double Q Learning need to reduce bias in Q-Learning and discount factor is a factor that affect Q-value the most)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**3.1 Results of hyperparameter adjustment**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will adjust 3 hyperparameter in the equation of q-value of each algorithm (MC, SARSA, Q-Learning Double Q-Learning) that are discount factor, learning rate, and epsilon decay and analyze how these hyperparater affect the learning process and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**Discount Factor**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, 3, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = **vary**\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `discount factor` and analyze how each algorithm performs in different `discount factor` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px; text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"picture/discount_factor_MC.png\" alt=\"Image 1\" width=\"100%\">\n",
    "        <figcaption>Monte Carlo (Sampling, On-policy)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"picture/discount_factor_SARSA.png\" alt=\"Image 2\" width=\"100%\">\n",
    "        <figcaption>SARSA (Bootstraping, On-policy)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"picture/discount_factor_Qlearning.png\" alt=\"Image 3\" width=\"100%\">\n",
    "        <figcaption>Q Learning (Bootstraping, Off-policy)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"picture/discount_factor_doubleQ.png\" alt=\"Image 4\" width=\"100%\">\n",
    "        <figcaption>Double Q Learning (Bootstraping, Off-policy)</figcaption>\n",
    "    </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Monte Carlo, no matter what the discount factor is, it can still learn at a similar speed, probably because sampling allows it to learn a wide range of states and at a faster rate. For Q-Learning, it can be seen that the higher the discount factor, the better it learns, because the overestimation of this algorithm pushes the optimal q-value to increase faster. However, Double Q-Learning and SARSA may require choosing an appropriate discount factor. If it is too high, it will cause bias, or if it is too low, it may cause slow learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `discount factor` and analyze how each algorithm performs under each `discount factor` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Double_Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 10\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Double_Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 11\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Double_Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 12\" style=\"width: 100%; height: auto;\">    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Monte Carlo, no matter what the discount factor is, the pole will oscillate quite a lot. For SARSA and Q-Learning, the pole will be quite stable at high learning rates, in contrast to Double Q-Learning, the pole will be stable at low discount factors. The overall reason may be that each algorithm learns in a different way, so it is necessary to find an appropriate discount factor for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**Learning Rate**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, 3, 3]  \n",
    "\n",
    "- learning_rate = **vary**  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `learning rate` and analyze how each algorithm performs in different `learning rate` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    ".caption {\n",
    "    font-size: 16px;\n",
    "    color: rgb(255,255,255);\n",
    "    margin-top: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/learning_rate_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/learning_rate_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo learning rate is not examined because it is a function of the number of times the states are encountered, and is not a hyperparameter. For SARSA, the higher the learning rate, the better it learns, possibly because higher learning rates induce the best actions. But with Q-Learning, it has the opposite effect. The higher the learning rate, the slower the learning. This may be because the learning rate is too high and does not reach the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `learning rate` and analyze how each algorithm performs under each `learning rate` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.05_0.9996_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.1_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_3_3_0.05_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_3_3_0.1_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SARSA, the pole is relatively stable at learning rate of 0.1, but for Q-Learning, pole is stable at learning rate of 0.25, which is not the value that gives the highest reward for both algorithms, probably because the actions chosen during train and play are different, resulting in different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**Epsilon Decay**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, 3, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = **vary** \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different epsilon decay and analyze how each algorithm performs in different epsilon decay and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    ".caption {\n",
    "    font-size: 16px;\n",
    "    color: rgb(255,255,255);\n",
    "    margin-top: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/epsilon_MC.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**MC (Sampling, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/epsilon_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/epsilon_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Monte Carlo and Q-Learning, you can see that the higher the epsilon decay, the slower the learning process. This is because a higher value of this value increases the opportunity for exploration. However, these two algorithms learn quickly, finding the best action quickly, but sometimes still need to explore, which slows down the reward. But with SARSA, we have to choose the right value of epsilon decay. If it is too low, it will learn poorly and not find the best action. If it is too high, it will miss the best action selection, causing slow learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `epsilon decay` and analyze how each algorithm performs under each `epsilon decay` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9985_0.5.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.999_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9985_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.999_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_3_3_0.25_0.9985_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_3_3_0.25_0.999_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, each algorithm produces different results when adjusting for epsilon decay. The best value is an epsilon decay of 0.999, which keeps the pole relatively steady compared to other epsilon decay values, probably because this is the value that produces the best action without learning too much or too little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**3.2 Performance of each Algorithm given same configulation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, 3, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = **vary**\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightgreen\">**In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different discount factor and analyze which algorithm performs the best and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    ".caption {\n",
    "    font-size: 16px;\n",
    "    color: rgb(255,255,255);\n",
    "    margin-top: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/discount0.3.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**Discount Factor 0.3**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/discount0.5.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Discount Factor 0.5**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/discount0.9.png\" alt=\"คำอธิบายรูปที่ 3\">\n",
    "    <div class=\"caption\">**Discount Factor 0.9**</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo** learns faster than all other algorithms. However, the reward value is highly volatile because it is learned by sampling each episode and then updating the Q-value. Also, because the experience from each episode is different, the volatility increases according to the difference in experience.\n",
    "\n",
    "**SARSA** learns slower than all other algorithms because it is a bootstrapping algorithm that updates the Q-value state with the current state. It seems to be a safe learning method and has less chance of finding extreme states. If the number of episodes is increased, it tends to learn better slowly.\n",
    "\n",
    "**Q Learning** is an algorithm that, when learned continuously, has a higher reward than other algorithms, and the reward received gradually increases steadily. At a discount factor of 0.9, Q-Learning learns faster than Double Q-Learning in the beginning. Both of these scenarios are because Q-learning computes $max_{⁡Q}(s',a')$ , which helps it focus on the path with the highest expected reward. If γ is high, the optimal choice may have to wait for a future reward, so Q-learning can find the optimal policy faster.\n",
    "\n",
    "**Double Q Learning** does not learn as fast as Q-Learning because Double Q-learning adjusts Q using two separate tables, reducing overestimation (a problem with Q-Learning) at discount factors of 0.3 and 0.5. Double Q-Learning learns faster than Q-Learning initially because its focus on immediate rewards makes overestimation less effective because the algorithm cannot estimate the future as much.\n",
    "\n",
    "<font color=\"orange\">Therefore, in the context of the less complex such as CartPole problem, Monte Carlo is the best algorithm because if this algorithm sampling the entire of possible state fast, that lead to learn fast.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightgreen\">**In term of deployment performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with different discount factor and analyze which algorithm performs the best and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">   \n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">     \n",
    "    <img src=\"plots/Double_Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.3.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">  \n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">  \n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">      \n",
    "    <img src=\"plots/Double_Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">        \n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 10\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 11\" style=\"width: 100%; height: auto;\">   \n",
    "    <img src=\"plots/Double_Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.9.png\" alt=\"Image 12\" style=\"width: 100%; height: auto;\">   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo** pole wobbles more than the other algorithms, probably because q_value updates are not as frequent as the other algorithms. Choosing q_value this time may not be the best action.\n",
    "\n",
    "**SARSA** pole doesn't oscillate as much as Monte Carlo, but there is still a lot of oscillation. This may be because this algorithm learns slowly and may not have found the best action for each state.\n",
    "\n",
    "**Q Learning** There are periods where the pole is still and periods where it vibrates a lot. This may be due to excessive bias in one of the actions.\n",
    "\n",
    "**Double Q Learning** Most poles are relatively stable, but if they shake too much, they will immediately go out of bounds. You may want to adjust the discount factor for this algorithm a little bit to improve its efficiency.\n",
    "\n",
    "<font color=\"orange\">Therefore, in the context of less complex such as the CartPole problem, Double Q-Learning is the best algorithm because this algorithm maintains CartPole for longer than other algorithms with a low discount factor.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**3.3 Results of resolutions of the action space and observation space adjustment**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will adjust resolutions of the action space and observation space of each algorithm (MC, SARSA, Q-Learning Double Q-Learning) and analyze how these hyperparater affect the learning process and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**num of action**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = **vary**\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, 3, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `num of action` and analyze how each algorithm performs in different `num of action` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/num_action_MC.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**MC (Sampling, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/num_action_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/num_action_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all algorithm, Different num_actions can make CartPole learn almost the same, This may be because the action is a force acting on the cart, which may not affect the goal of this task, which is to stabilize the pole. But it may be because of other factors such as good discrete_state_weight or action_range may not be suitable yet, so we do not see much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `num of action` and analyze how each algorithm performs under each `num of action` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_5_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_13_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_5_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_13_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_5_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_13_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different num of actions affect each algorithm differently, but Q-Learning seems to handle different num of actions better than the other algorithms, look at the stability of the pole. But for SARSA and Monte Carlo, if the num of actions is too large, the pole cannot stabilize as well as Q-Learning, probably because Q-Learning finds the best action for each state, but other algorithms may not find it yet. The higher the num of actions, the less chance there is to find it because it has to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**discretize_state_weight[0] (pose_cart)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [**vary**, 11, 3, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `discretize_state_weight[0] (pose_cart)` and analyze how each algorithm performs in different `discretize_state_weight[0] (pose_cart)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight0_MC.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**MC (Sampling, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight0_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight0_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SARSA and Q-Learning, Different pose_cart discretize_state_weight can make CartPole learns approximately the same, but the higher the weight, the slower the learning. This may be because when there are more possible states, the agent needs more time to learn, and there may be a situation where the agent encounters only the same states and does not learn the rare states, such as the state where Cart is on the edge near the point where it terminates.\n",
    "\n",
    "For Monte Carlo, Different pose_cart discretize_state_weight can make CartPole learns approximately the same, but the higher the weight, the higher the learning. This may be because Sampling helps agent to find a variety of states and learn faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `discretize_state_weight[0] (pose_cart)` and analyze how each algorithm performs under each `discretize_state_weight[0] (pose_cart)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_11_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_17_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_11_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_17_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_11_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_17_3_3_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the pattern of results with this weight adjustment is not captured, but it is clear that SARSA cannot stabilize the pole until the end of the episode, and even if it does, the pole is not stable. This may be due to the problem of the algorithm learning too many safe paths. However, Q-Learning can handle a variety of weights. It may be because this algorithm has already found the best action. For the unclear Monte Carlo pattern, it may be because of the sampling, sometimes it finds the best action, sometimes it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**discretize_state_weight[1] (pose_pole)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, **vary**, 3, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `discretize_state_weight[1] (pose_pole)` and analyze how each algorithm performs in different `discretize_state_weight[1] (pose_pole)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight1_MC.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**MC (Sampling, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight1_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight1_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all algorithm, Different pose_pole discretize_state_weight can make CartPole learns approximately the same, but the higher the weight, the higher the learning. This may be because this weight affect the learning of agent the most that goal is stabilize the pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `discretize_state_weight[1] (pose_pole)` and analyze how each algorithm performs under each `discretize_state_weight[1] (pose_pole)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_5_3_3_0.25_0.9996_0.5.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_17_3_3_0.25_0.9996_0.5.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_5_3_3_0.25_0.9996_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_17_3_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_5_3_3_0.25_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_17_3_3_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that for any algorithm, the larger this weight is, the more stable the pole will be, because this weight is the main factor related to the goal of this task itself, which is to stabilize the pole at the upright position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**discretize_state_weight[2] (vel_cart)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, **vary**, 3]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `discretize_state_weight[2] (vel_cart)` and analyze how each algorithm performs in different `discretize_state_weight[2] (vel_cart)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight2_MC.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**MC (Sampling, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight2_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight2_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SARSA and Q-Learning, From the graph, it tends to show that the higher the value of weight[2], the better CartPole learns, but it can be observed that at weight[2] 6 and 9, the average reward values ​​are close to each other, possibly because the higher granularity in dividing possible states has less effect on CartPole's learning, but overall the graphs are close to each other. Therefore, it can be concluded that weight[2] has a relatively small effect on learning.\n",
    "\n",
    "For Monte Carlo, this weight lead the average reward values ​​are close to each other. This may be because Sampling helps agent to find a variety of states and learn faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `discretize_state_weight[2] (vel_cart)` and analyze how each algorithm performs under each `discretize_state_weight[2] (vel_cart)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_6_3_0.25_0.9996_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_9_3_0.25_0.9996_0.5.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_6_3_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_9_3_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_6_3_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_9_3_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the higher this weight increases, the more stable the pole becomes for all algorithms. There may be times when it seems unable to stabilize but can still come back, depending on the starting position of each episode. If the starting position is a position where the algorithm does not perform well, For example, Q-Learning with this weight equal to 6 will cause a oscillation throughout the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"lightblue\">**discretize_state_weight[3] (vel_pole)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter I used for every algorithm**\n",
    "- num_of_action = 9\n",
    "\n",
    "- action_range = [-12, 12]  \n",
    "\n",
    "- discretize_state_weight = [5, 11, 3, **vary**]  \n",
    "\n",
    "- learning_rate = 0.25  \n",
    "\n",
    "- n_episodes = 5000  \n",
    "\n",
    "- start_epsilon = 1.0  \n",
    "\n",
    "- epsilon_decay = 0.9996  \n",
    "\n",
    "- final_epsilon = 0.05  \n",
    "\n",
    "- discount = 0.5\n",
    "\n",
    "**Clipping value**\n",
    "- pose_cart_bound = [-3, 3]\n",
    "\n",
    "- pose_pole_bound = [float(np.deg2rad(-24.0)), float(np.deg2rad(24.0))]\n",
    "\n",
    "- vel_cart_bound = [-10, 10]\n",
    "\n",
    "- vel_pole_bound = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In terms of learning efficiency**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the average step and average reward every 100 episode for each algorithm with different `discretize_state_weight[3] (vel_pole)` and analyze how each algorithm performs in different `discretize_state_weight[3] (vel_pole)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".image-container {\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".image-container img {\n",
    "    width: 100%; /* ใช้ 100% เพื่อให้ขยายเต็มพื้นที่ */\n",
    "    max-width: 1200px; /* กำหนดขนาดสูงสุดให้รูปใหญ่ขึ้น */\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight3_MC.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**MC (Sampling, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight3_SARSA.png\" alt=\"คำอธิบายรูปที่ 1\">\n",
    "    <div class=\"caption\">**SARSA (Bootsrtaping, On-Policy)**</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"picture/weight3_Qlearning.png\" alt=\"คำอธิบายรูปที่ 2\">\n",
    "    <div class=\"caption\">**Q Learning (Bootsrtaping, Off-Policy)**</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SARSA and Q-Learning, From the graph, it tends to show that the higher the value of weight[3], the better CartPole learns, but it can be observed that at weight[3] 6 and 9, the average reward values ​​are similar, possibly because the higher granularity in dividing possible states has less effect on CartPole's learning. But finally, at the end of episode 5000, the rewards that CartPole received on average were similar, so it can be concluded that it can be concluded that weight[3] has a relatively small effect on learning.\n",
    "\n",
    "For Monte Carlo, this weight lead the average reward values ​​are close to each other. This may be because Sampling helps agent to find a variety of states and learn faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=\"lightgreen\">In term of deployment performance</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show the pole position in the maximum time that each algorithm can stabilize with a different `discretize_state_weight[3] (vel_pole)` and analyze how each algorithm performs under each `discretize_state_weight[3] (vel_pole)` and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_6_0.25_0.9996_0.5.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_9_12.0_5_11_3_9_0.25_0.9996_0.5.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_6_0.25_0.9996_0.5.png\" alt=\"Image 5\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/SARSA_9_12.0_5_11_3_9_0.25_0.9996_0.5.png\" alt=\"Image 6\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12_5_11_3_3_0.25_0.9996_0.5.png\" alt=\"Image 7\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_3_6_0.25_0.9996_0.5.png\" alt=\"Image 8\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/Q_Learning_9_12.0_5_11_3_9_0.25_0.9996_0.5.png\" alt=\"Image 9\" style=\"width: 100%; height: auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Monte Carlo and SARSA, the more this weight increases, the better the pole stabilizes, but Q-Learning shows the opposite trend, probably because Q-Learning cannot choose the best action like other algorithms because it overestimates other actions, while other algorithms find the best action if the weight increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
