{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3: Function-based RL**\n",
    "#### **Created by 65340500058 Anuwit Intet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives:**\n",
    "\n",
    "- Understand how function approximation works and how to implement it.\n",
    "\n",
    "- Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "- Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "- Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "- Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 1: Understanding the Algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you have to implement 4 different function approximation-based RL algorithms:\n",
    "\n",
    "- Linear Q-Learning\n",
    " \n",
    "- Deep Q-Network (DQN)\n",
    "\n",
    "- REINFORCE algorithm\n",
    "\n",
    "- One algorithm chosen from the following Actor-Critic methods:\n",
    "\n",
    "    - Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "    - Advantage Actor-Critic (A2C)\n",
    "\n",
    "    - Proximal Policy Optimization (PPO)\n",
    "    \n",
    "    - Soft Actor-Critic (SAC)\n",
    "\n",
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it follows a value-based, policy-based, or Actor-Critic approach\n",
    "\n",
    "- the type of policy it learns (stochastic or deterministic)\n",
    "\n",
    "- the type of observation space and action space (discrete or continuous)\n",
    "\n",
    "- how each advanced RL method balances exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Linear Q-Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- About Linear Q-Learning\n",
    "  - Linear Q-Learning is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "  - This algorithm uses the deterministic policy because Linear Q-Learning uses a Œµ-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "  - Linear Q-Learning is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max‚Å°_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "  - To balance Exploration vs Exploitation, Linear Q-Learning uses a Œµ-greedy policy, i.e. random action with probability Œµ and greedy action with probability 1‚àíŒµ.\n",
    "\n",
    "- In Linear Q-Learning, Q-Function is estimate by\n",
    "\n",
    "  $$\n",
    "  Q(s,a) = \\phi(s,a)^T w\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  - $\\phi(s,a)$ is feature vector of state-action pair  \n",
    "  - $w$ is weight vector\n",
    "\n",
    "- And update weight by this,\n",
    "\n",
    "  $$\n",
    "  w \\leftarrow w + \\alpha \\cdot \\delta \\cdot \\phi(s, a)\n",
    "  $$\n",
    "\n",
    "  where: \n",
    "  - $\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$ is TD error\n",
    "  - $\\alpha$ is learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Linear Q-Learning ‡∏ö‡∏ô CartPole**\n",
    "\n",
    "- **State**: ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏ô‡∏≤‡∏î 4:  \n",
    "  $$\n",
    "  s = [x, \\dot{x}, \\theta, \\dot{\\theta}]\n",
    "  $$\n",
    "- **Action space**: ‡∏°‡∏µ 2 ‡∏Ñ‡πà‡∏≤ (discrete):\n",
    "  - `0` = push cart ‡πÑ‡∏õ‡∏ã‡πâ‡∏≤‡∏¢\n",
    "  - `1` = push cart ‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤\n",
    "\n",
    "---\n",
    "\n",
    "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì $Q(s, a)$ ‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:\n",
    "\n",
    "$$\n",
    "Q(s, a) = w_a^T s\n",
    "$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:\n",
    "- $w_0$, $w_1$ ‡∏Ñ‡∏∑‡∏≠ weight vectors ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö action 0 ‡πÅ‡∏•‡∏∞ 1 ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö\n",
    "- ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô matrix $W \\in \\mathbb{R}^{2 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "- $s = [0.0, 0.5, 0.05, -0.2]$\n",
    "- $W = \\begin{bmatrix} 0.1 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.1 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action $a = 1$\n",
    "- ‡πÑ‡∏î‡πâ reward $r = 1$\n",
    "- next state: $s' = [0.01, 0.55, 0.045, -0.18]$\n",
    "- $\\alpha = 0.1$, $\\gamma = 0.99$\n",
    "\n",
    "---\n",
    "\n",
    "**1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì $Q(s, a)$**\n",
    "\n",
    "$$\n",
    "Q(s, a=1) = w_1^T s = 0.0*0.0 + 0.1*0.5 + 0.0*0.05 + 0.0*(-0.2) = 0.05\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì $\\max_{a'} Q(s', a')$**\n",
    "\n",
    "$$\n",
    "Q(s', 0) = w_0^T s' = 0.1*0.01 = 0.001 \\\\\n",
    "Q(s', 1) = w_1^T s' = 0.1*0.55 = 0.055 \\\\\n",
    "\\Rightarrow \\max_{a'} Q(s', a') = 0.055\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì TD Error**\n",
    "\n",
    "$$\n",
    "\\delta = r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\\\\n",
    "= 1 + 0.99 \\cdot 0.055 - 0.05 = 1.00445\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**4. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï weight**\n",
    "\n",
    "‡πÄ‡∏â‡∏û‡∏≤‡∏∞ $w_1$:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 + \\alpha \\cdot \\delta \\cdot s \\\\\n",
    "= [0.0, 0.1, 0.0, 0.0] + 0.1 \\cdot 1.00445 \\cdot [0.0, 0.5, 0.05, -0.2] \\\\\n",
    "= [0.0, 0.1502, 0.005, -0.0201]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Q-Network (DQN)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî∏ DQN ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "\n",
    "DQN ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ deep neural network ‡∏°‡∏≤‡πÅ‡∏ó‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á Q ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô: $Q(s,a;Œ∏)$\n",
    "\n",
    "\n",
    "‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:\n",
    "\n",
    "- Œ∏ ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á neural network\n",
    "\n",
    "- input = ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ s (‡πÄ‡∏ä‡πà‡∏ô ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏° Atari)\n",
    "\n",
    "- output = ‡∏Ñ‡πà‡∏≤ Q ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å action\n",
    "\n",
    "üîß ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å DQN ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏´‡∏•‡∏±‡∏Å 3 ‡∏≠‡∏¢‡πà‡∏≤‡∏á:\n",
    "\n",
    "- Experience Replay\n",
    "    - ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå $(s, a, r, s', done)$ ‡∏•‡∏á‡πÉ‡∏ô buffer\n",
    "    - ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏∏‡πà‡∏° mini-batch ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ù‡∏∂‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠ decorrelate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "- Target Network\n",
    "    - ‡πÉ‡∏ä‡πâ network ‡πÅ‡∏¢‡∏Å‡∏≠‡∏µ‡∏Å‡∏≠‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠ target network QtargetQtarget‚Äã ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô target:y=r+Œ≥max‚Å°a‚Ä≤Qtarget(s‚Ä≤,a‚Ä≤)\n",
    "    - ‡πÅ‡∏•‡πâ‡∏ß update ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ network ‡∏´‡∏•‡∏±‡∏Å Q ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞\n",
    "\n",
    "- Fixed Action Space\n",
    "    - Action space ‡∏ï‡πâ‡∏≠‡∏á discrete ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì max‚Å°aQ(s,a)maxa‚ÄãQ(s,a) ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢\n",
    "\n",
    "üîÅ ‡∏™‡∏£‡∏∏‡∏õ Training Loop:\n",
    "\n",
    "- ‡∏£‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ s\n",
    "\n",
    "- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏à‡∏≤‡∏Å Q-network ‡∏î‡πâ‡∏ß‡∏¢ $\\epsilon$-greedy\n",
    "\n",
    "- ‡∏ó‡∏≥ action ‚Üí ‡πÑ‡∏î‡πâ reward r, next state s‚Ä≤\n",
    "\n",
    "- ‡πÄ‡∏Å‡πá‡∏ö transition ‡∏•‡∏á replay buffer\n",
    "\n",
    "- ‡∏™‡∏∏‡πà‡∏° batch ‡∏à‡∏≤‡∏Å buffer ‚Üí ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì TD error\n",
    "\n",
    "- ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Œ∏ ‡πÇ‡∏î‡∏¢ gradient descent\n",
    "\n",
    "‚úÖ ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏¢‡πà‡∏≠‡∏¢\n",
    "1. ‚ùì It follows a value-based, policy-based, or Actor-Critic approach?\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
    "üîπ DQN ‡πÄ‡∏õ‡πá‡∏ô value-based approach ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πà‡∏≤ Q(s,a)Q(s,a) ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÅ‡∏•‡πâ‡∏ß derive policy ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ Q ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "\n",
    "2. ‚ùì The type of policy it learns (stochastic or deterministic)?\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
    "üî∏ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏∑‡∏≠ deterministic\n",
    "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏î‡πâ‡∏ß‡∏¢:\n",
    "a‚àó=arg‚Å°max‚Å°aQ(s,a)\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á training ‡∏à‡∏∞‡πÉ‡∏ä‡πâ: $\\epsilon$ -greedy policy ‚Üí stochastic ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ explore\n",
    "\n",
    "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô:\n",
    "\n",
    "Phase\tType of policy\n",
    "Training\tStochastic (for exploration)\n",
    "Deployment\tDeterministic (greedy action)\n",
    "\n",
    "3. ‚ùì The type of observation space and action space?\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
    "- Observation Space\tContinuous ‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û, ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏Ø‡∏•‡∏Ø (‡∏ú‡πà‡∏≤‡∏ô neural net)\n",
    "- Action Space\tDiscrete ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ DQN ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì $\\max_a Q(s, a)$)\n",
    "\n",
    "‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö continuous action ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ DDPG, SAC, ‡∏´‡∏£‡∏∑‡∏≠ TD3 ‡πÅ‡∏ó‡∏ô\n",
    "\n",
    "4. ‚ùì How does this method balance exploration and exploitation?\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
    "- DQN ‡πÉ‡∏ä‡πâ Œµ-greedy strategy ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ balance:\n",
    "\n",
    "    - ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏∏‡πà‡∏° (explore) = $\\epsilon$\n",
    "\n",
    "    - ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (exploit) = $1 - \\epsilon$\n",
    "\n",
    "- ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏±‡∏ö $\\epsilon$ ‡∏ï‡∏≤‡∏° schedule ‡πÄ‡∏ä‡πà‡∏ô:\n",
    "\n",
    "    - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà $\\epsilon = 1.0$\n",
    "\n",
    "    - ‡∏•‡∏î‡∏•‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢‡∏ñ‡∏∂‡∏á‡πÄ‡∏ä‡πà‡∏ô $\\epsilon = 0.1$ ‡∏´‡∏£‡∏∑‡∏≠ $\\epsilon = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**REINFORCE algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Deterministic Policy Gradient (DDPG)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Advantage Actor-Critic (A2C)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Proximal Policy Optimization (PPO)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Proximal Policy Optimization (PPO)\n",
    "\n",
    "## ‚úÖ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á PPO\n",
    "\n",
    "PPO ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô Policy Gradient algorithms ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏î‡∏¢ OpenAI ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠:\n",
    "\n",
    "- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ (Policy) ‡∏°‡∏µ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏†‡∏≤‡∏û (stable)\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å policy ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (on-policy)\n",
    "- ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö ‚Äú‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‚Äù ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ performance ‡πÅ‡∏¢‡πà‡∏•‡∏á\n",
    "\n",
    "---\n",
    "\n",
    "## üèó ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á PPO\n",
    "\n",
    "PPO ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á **Actor-Critic** ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
    "\n",
    "- **Actor**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ $\\pi_\\theta(a|s)$ ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action\n",
    "- **Critic**: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô value ‡∏Ç‡∏≠‡∏á state ‡∏´‡∏£‡∏∑‡∏≠ action ‡πÄ‡∏ä‡πà‡∏ô $V(s)$ ‡∏´‡∏£‡∏∑‡∏≠ $Q(s,a)$ ‚Üí ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì advantage\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á PPO (Step-by-Step)\n",
    "\n",
    "### 1. **Collect Trajectories**\n",
    "- ‡πÉ‡∏´‡πâ agent ‡∏ß‡∏¥‡πà‡∏á‡πÉ‡∏ô environment ‡∏ï‡∏≤‡∏° policy ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n",
    "- ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: $(s_t, a_t, r_t, \\log \\pi(a_t|s_t), done)$\n",
    "- ‡∏£‡∏≠‡∏à‡∏ô‡πÑ‡∏î‡πâ rollout ‡∏Ñ‡∏£‡∏ö (‡πÄ‡∏ä‡πà‡∏ô 2048 steps ‡∏´‡∏£‡∏∑‡∏≠ 1 episode)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Compute Returns & Advantages**\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Monte Carlo return** ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ **GAE (Generalized Advantage Estimation)**:\n",
    "  \n",
    "  ```math\n",
    "  A_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + ... ‚âà R_t - V(s_t)\n",
    "\n",
    "### 3. Surrogate Objective with Clipping\n",
    "\n",
    "- PPO ‡πÉ‡∏ä‡πâ surrogate loss function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡∏≠‡∏á‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢:\n",
    "- rt(Œ∏)=œÄŒ∏(at‚à£st)œÄŒ∏old(at‚à£st)\n",
    "- LCLIP(Œ∏)=Et[min‚Å°(rt(Œ∏)At,clip(rt(Œ∏),1‚àíœµ,1+œµ)At)]\n",
    "- ‡∏ñ‡πâ‡∏≤ $r_t$ ‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏à‡∏≤‡∏Å 1 ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å clip ‡πÑ‡∏ß‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô policy ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "### 4. Update Policy and Value Function\n",
    "\n",
    "- ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Actor ‡∏î‡πâ‡∏ß‡∏¢ loss ‡∏à‡∏≤‡∏Å surrogate objective\n",
    "\n",
    "- ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Critic ‡∏î‡πâ‡∏ß‡∏¢ MSE loss ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á $V(s_t)$ ‡∏Å‡∏±‡∏ö return\n",
    "\n",
    "### 5. Repeat Training for Multiple Epochs\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• rollout ‡πÄ‡∏î‡∏¥‡∏°‡∏ù‡∏∂‡∏Å‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö (‡πÄ‡∏ä‡πà‡∏ô 4-10 epochs)\n",
    "\n",
    "- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ sample efficient ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Soft Actor-Critic (SAC)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 2: Setting up Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous homework, you will implement a common components that will be the same in most of the function approximation-based RL in the RL_base_function.py.The core components should include, but are not limited to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**1. RL Base class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - Number of actions: The total number of discrete actions available to the agent.\n",
    "\n",
    "        - Action range: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "        - Discretize state weight: Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "        - Learning rate: Determines how quickly the model updates based on new information.\n",
    "\n",
    "        - Initial epsilon: The starting probability of taking a random action in an Œµ-greedy policy.\n",
    "\n",
    "        - Epsilon decay rate: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "        - Final epsilon: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "        - Discount factor: A coefficient (Œ≥) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "        - Buffer size: Maximum number of experiences the buffer can hold.\n",
    "\n",
    "        - Batch size: Number of experiences to sample per batch.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - scale_action(): scale the action (if it is computed from the sigmoid or softmax function) to the proper length.\n",
    "\n",
    "        - decay_epsilon(): Decreases epsilon over time and returns the updated value.\n",
    "\n",
    "- Additional details about these functions are provided in the class file. You may also implement additional functions for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**scale_action()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_action(self, action):\n",
    "    \"\"\"\n",
    "    Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "    Args:\n",
    "        action (int): Discrete action in range [0, n].\n",
    "        n (int): Number of discrete actions (inclusive range from 0 to n).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled action tensor.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Unpack the minimum and maximum values of the action range\n",
    "    action_min, action_max = self.action_range\n",
    "\n",
    "    # Scale the discrete action index (0 to num_of_action-1) to a continuous value within [action_min, action_max]\n",
    "    scaled = action_min + (action / (self.num_of_action - 1)) * (action_max - action_min)\n",
    "\n",
    "    # Check if the scaled value is already a torch.Tensor\n",
    "    if isinstance(scaled, torch.Tensor):\n",
    "        # If yes, detach it from any computation graph and convert to float32\n",
    "        return scaled.clone().detach().to(dtype=torch.float32)\n",
    "    else:\n",
    "        # Otherwise, convert it into a torch.Tensor of type float32\n",
    "        return torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**decay_epsilon()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self):\n",
    "    \"\"\"\n",
    "    Decay epsilon value to reduce exploration over time.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Decay the exploration rate (epsilon) by multiplying with epsilon_decay,\n",
    "    # but ensure it doesn't go below the minimum value (final_epsilon)\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**2. Replay Buffer Class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- A class use to store state, action, reward, next state, and termination status from each timestep in episode to use as a dataset to train neural networks. This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - memory: FIFO buffer to store the trajectory within a certain time window.\n",
    "\n",
    "        - batch_size: Number of data samples drawn from memory to train the neural network.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - add(): Add state, action, reward, next state, and termination status to the FIFO buffer. Discard the oldest data in the buffer\n",
    "\n",
    "        - sample(): Sample data from memory to use in the neural network training.\n",
    "\n",
    "    - <font color=\"orange\">**Note that some algorithms may not use all of the data mentioned above to train the neural network.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**add()**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**sample()**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**3. Algorithm folder**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This folder should include:\n",
    "\n",
    "    - Linear Q Learning class\n",
    "\n",
    "    - Deep Q-Network class\n",
    "\n",
    "    - REINFORCE Class\n",
    "\n",
    "    - One class chosen from the Part 1.\n",
    "\n",
    "- Each class should inherit from the RL Base class in RL_base_function.py and include:\n",
    "\n",
    "    - A constructor which initializes the same variables as the class it inherits from.\n",
    "\n",
    "    - Superclass Initialization (super().__init__()).\n",
    "\n",
    "    - An update() function that updates the agent‚Äôs learnable parameters and advances the training step.\n",
    "\n",
    "    - A select_action() function select the action according to current policy.\n",
    "\n",
    "    - A learn() function that train the regression or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Linear Q-Learning class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Deep Q-Network (DQN) class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**REINFORCE class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**DDPG class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**A2C class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**PPO class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**SAC class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 3: Trainning & Playing to stabilize Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the training loop in train script and main() in the play script (in the \"Can be modified\" area of both files). Additionally, you must collect data, analyze results, and save models for evaluating agent performance.\n",
    "\n",
    "- Training the Agent\n",
    "\n",
    "    - Stabilizing Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task SwingUp-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "- Playing\n",
    "\n",
    "    - Stabilize Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ``` \n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task SwingUp-Isaac-Cartpole-v0 \n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**train.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**play.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 4: Evaluate Cart-Pole Agent performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must evaluate the agent's performance in terms of learning efficiency (i.e., how well the agent learns to receive higher rewards) and deployment performance (i.e., how well the agent performs in the Cart-Pole problem). Analyze and visualize the results to determine:\n",
    "\n",
    "- Which algorithm performs best?\n",
    "\n",
    "- Why does it perform better than the others?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of deployment performance**</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
