{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3: Function-based RL**\n",
    "#### **Created by 65340500058 Anuwit Intet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives:**\n",
    "\n",
    "- Understand how function approximation works and how to implement it.\n",
    "\n",
    "- Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "- Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "- Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "- Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 1: Understanding the Algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you have to implement 4 different function approximation-based RL algorithms:\n",
    "\n",
    "- Linear Q-Learning\n",
    " \n",
    "- Deep Q-Network (DQN)\n",
    "\n",
    "- REINFORCE algorithm\n",
    "\n",
    "- One algorithm chosen from the following Actor-Critic methods:\n",
    "\n",
    "    - Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "    - Advantage Actor-Critic (A2C)\n",
    "\n",
    "    - Proximal Policy Optimization (PPO)\n",
    "    \n",
    "    - Soft Actor-Critic (SAC)\n",
    "\n",
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it follows a value-based, policy-based, or Actor-Critic approach\n",
    "\n",
    "- the type of policy it learns (stochastic or deterministic)\n",
    "\n",
    "- the type of observation space and action space (discrete or continuous)\n",
    "\n",
    "- how each advanced RL method balances exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Linear Q-Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Linear Q-Learning**\n",
    "\n",
    "  - Linear Q-Learning is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "  - This algorithm uses the deterministic policy because Linear Q-Learning uses a ε-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "  - Linear Q-Learning is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max⁡_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "  - To balance Exploration vs Exploitation, Linear Q-Learning uses a ε-greedy policy, i.e. random action with probability ε and greedy action with probability 1−ε.\n",
    "\n",
    "- **In Linear Q-Learning, Q-Function is estimate by**\n",
    "\n",
    "  $$\n",
    "  Q(s,a) = \\phi(s,a)^T w\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  - $\\phi(s,a)$ is feature vector of state-action pair  \n",
    "  - $w$ is weight vector\n",
    "\n",
    "- **And update weight by this,**\n",
    "\n",
    "  $$\n",
    "  w \\leftarrow w + \\alpha \\cdot \\delta \\cdot \\phi(s, a)\n",
    "  $$\n",
    "\n",
    "  where: \n",
    "  - $\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$ is TD error\n",
    "  - $\\alpha$ is learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of Linear Q-Learning in CartPole**\n",
    "\n",
    "- **State**: Vector of size 4:  \n",
    "  $$\n",
    "  s = [x, \\dot{x}, \\theta, \\dot{\\theta}]\n",
    "  $$\n",
    "- **Action space**: 2 Actions (discrete):\n",
    "\n",
    "  - `0` = push cart to left\n",
    "\n",
    "  - `1` = push cart to right\n",
    "\n",
    "---\n",
    "\n",
    "We will approximate $Q(s, a)$ with a linear function like this:\n",
    "\n",
    "$$\n",
    "Q(s, a) = w_a^T s\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_0$, $w_1$ are weight vectors for action 0 and 1 respectively\n",
    "- or combined into a matrix $W \\in \\mathbb{R}^{2 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "- $s = [0.0, 0.5, 0.05, -0.2]$\n",
    "- $W = \\begin{bmatrix} 0.1 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.1 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- Choose action $a = 1$\n",
    "- Acheive reward $r = 1$\n",
    "- next state: $s' = [0.01, 0.55, 0.045, -0.18]$\n",
    "- $\\alpha = 0.1$, $\\gamma = 0.99$\n",
    "\n",
    "\n",
    "**1. Calculate $Q(s, a)$**\n",
    "\n",
    "$$\n",
    "Q(s, a=1) = w_1^T s = 0.0*0.0 + 0.1*0.5 + 0.0*0.05 + 0.0*(-0.2) = 0.05\n",
    "$$\n",
    "\n",
    "**2. Calculate $\\max_{a'} Q(s', a')$**\n",
    "\n",
    "$$\n",
    "Q(s', 0) = w_0^T s' = 0.1*0.01 = 0.001 \\\\\n",
    "Q(s', 1) = w_1^T s' = 0.1*0.55 = 0.055 \\\\\n",
    "\\Rightarrow \\max_{a'} Q(s', a') = 0.055\n",
    "$$\n",
    "\n",
    "**3. Calculate TD Error**\n",
    "\n",
    "$$\n",
    "\\delta = r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\\\\n",
    "= 1 + 0.99 \\cdot 0.055 - 0.05 = 1.00445\n",
    "$$\n",
    "\n",
    "**4. Update weight**\n",
    "\n",
    "$w_1$ only:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 + \\alpha \\cdot \\delta \\cdot s \\\\\n",
    "= [0.0, 0.1, 0.0, 0.0] + 0.1 \\cdot 1.00445 \\cdot [0.0, 0.5, 0.05, -0.2] \\\\\n",
    "= [0.0, 0.1502, 0.005, -0.0201]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Q-Network (DQN)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Deep Q-Network**\n",
    "\n",
    "    - Deep Q-Network has the same property of Linear Q-Learning, Both has difference in term of complexity of network that from 1 layer neural network to deep neural network (more than 1 layer).\n",
    "\n",
    "    - Deep Q-Network is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "    - This algorithm uses the deterministic policy because Deep Q-Network uses a ε-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "    - Deep Q-Network is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max⁡_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "    - To balance Exploration vs Exploitation, Deep Q-Network uses a ε-greedy policy, i.e. random action with probability ε and greedy action with probability 1−ε.\n",
    "\n",
    "- DQN solves this problem by using a deep neural network to replace the Q-table with an approximate function: $Q(s,a;θ)$.\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - θ is the neural network parameter\n",
    "\n",
    "    - input = state s\n",
    "\n",
    "    - output = Q value for every action\n",
    "\n",
    "- DQN training consists of 2 main techniques:\n",
    "\n",
    "    - Experience Replay\n",
    "        - Store the experience $(s, a, r, s', done)$ in a buffer\n",
    "        - Then randomly select a mini-batch to train it\n",
    "\n",
    "    - Target Network\n",
    "        - Use a separate network called target network​ to calculate the target:$y=r+γmax_{⁡a′}Q_{target}(s′,a′)$\n",
    "        - Then update only the main network Q periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of DQN in CartPole**\n",
    "\n",
    "**1. Choose action with $\\epsilon$-greedy**\n",
    "\n",
    "- Suppose $\\epsilon = 0.1$ → Random chance = 10%\n",
    "\n",
    "- Luckily pick Random → Use Q-network to predict:\n",
    "$$\n",
    "Q(s, a=0) = 0.4,\\quad Q(s, a=1) = 0.6\n",
    "$$\n",
    "\n",
    "- Choose **action = 1** (right) because Q is highest\n",
    "\n",
    "**2. Send action to environment**\n",
    "\n",
    "- Got:\n",
    "\n",
    "  - reward = 1\n",
    "\n",
    "  - next_state = [0.06, 0.025, -0.015, 0.035]\n",
    "\n",
    "  - done = False (Not yet failed)\n",
    "\n",
    "**3. Store transition**\n",
    "\n",
    "- Store $(s, a=1, r=1, s', done=False)$ in the replay buffer.\n",
    "\n",
    "**4. Assume that the buffer is sufficient → Start training 1 round**\n",
    "\n",
    "- Suppose that we store the transition in the replay buffer and randomly get 2 mini-batch samples as follows:\n",
    "\n",
    "| Index | State (s)                  | Action (a) | Reward (r) | Next State (s′)             | Done  |\n",
    "|-------|----------------------------|------------|------------|------------------------------|--------|\n",
    "| 0     | [0.05, 0.02, -0.01, 0.03]  | 1          | 1.0        | [0.06, 0.025, -0.015, 0.035] | False  |\n",
    "| 1     | [-0.01, -0.03, 0.02, -0.02] | 0         | 1.0        | [0.00, -0.02, 0.01, -0.01]   | True   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Calculate Q(s, a) from policy network**\n",
    "\n",
    "Suppose the policy network gives:\n",
    "\n",
    "| Index | Q(s, a=0) | Q(s, a=1) |\n",
    "|-------|-----------|-----------|\n",
    "| 0 | 0.5 | 0.65 |\n",
    "| 1 | 0.6 | 0.55 |\n",
    "\n",
    "Get Q of the selected action:\n",
    "\n",
    "- Example 0: action = 1 → Q = 0.65\n",
    "\n",
    "- Example 1: action = 0 → Q = 0.6\n",
    "\n",
    "**Q(s,a) = [0.65, 0.6]**\n",
    "\n",
    "**Calculate Target Q(s′, a′) from target network**\n",
    "\n",
    "Suppose Target Network gives:\n",
    "\n",
    "| Index | Q(s′, a=0) | Q(s′, a=1) | Done |\n",
    "|-------|------------|------------|--------|\n",
    "| 0 | 0.4 | 0.6 | False |\n",
    "| 1 | -- | -- | True |\n",
    "\n",
    "→ max Q(s′) only for not done:\n",
    "\n",
    "- Index 0: max = 0.6 \n",
    "\n",
    "- Index 1: terminal → max = 0\n",
    "\n",
    "**max_next_q_values ​​= [0.6, 0.0]**\n",
    "\n",
    "**7. Calculate Target Q-value (Bellman target)**\n",
    "\n",
    "Use the formula:\n",
    "$$\n",
    "y = r + \\gamma \\cdot (1 - \\text{done}) \\cdot \\max Q(s', a')\n",
    "$$\n",
    "\n",
    "ให้ $\\gamma = 0.99$:\n",
    "\n",
    "- Index 0:  $y = 1.0 + 0.99 \\cdot 0.6 = 1.594$\n",
    "\n",
    "- Index 1:  $y = 1.0 + 0 = 1.0$\n",
    "\n",
    "**Target Q-values = [1.594, 1.0]**\n",
    "\n",
    "**8. Calculate Loss**\n",
    "\n",
    "Use the formula:\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\sum_i (Q(s_i, a_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "- $Q(s_i, a_i)$ is Q-value from policy network\n",
    "\n",
    "- $y_i$ is Q-value from target network\n",
    "\n",
    "Substitute the value:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\left[ (0.65 - 1.594)^2 + (0.6 - 1.0)^2 \\right] \\\\\n",
    "= \\frac{1}{2} \\left[ 0.891 + 0.16 \\right] = \\frac{1.051}{2} = \\mathbf{0.5255}\n",
    "$$\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Index | Q(s, a) | Target y | Loss per item |\n",
    "|-------|---------|----------|----------------|\n",
    "| 0     | 0.65    | 1.594    | 0.891          |\n",
    "| 1     | 0.6     | 1.0      | 0.160          |\n",
    "|       |         |          | Total = 1.051 |\n",
    "|       |         |          | Avg = 0.5255  |\n",
    "\n",
    "**Final Loss = 0.5255**\n",
    "\n",
    "**9. Do Gradient Descent**\n",
    "\n",
    "- Call `loss.backward()` to compute gradient\n",
    "\n",
    "- Call `optimizer.step()` to update policy network weights\n",
    "\n",
    "**This is where Q-network learns from the latest experience that is randomly selected from the replay buffer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**REINFORCE algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About MC-REINFORCE Algorithm**\n",
    "\n",
    "    - MC-REINFORCE (Monte Carlo REINFORCE) เป็นอัลกอริทึมพื้นฐานของ Policy Gradient Method ใช้แนวคิดการเรียนรู้ นโยบาย (policy) โดยตรงไม่ประมาณค่า Q-function แบบ Q-learning\n",
    "\n",
    "    - MC-REINFORCE เป็น Policy-based เพราะ REINFORCE ไม่เรียนรู้ Q-value หรือ V-value แต่เรียนรู้นโยบาย $\\pi_\\theta$ โดยตรง (pure policy gradient method)\n",
    "\n",
    "    - type of policy is Stochastic policy เพราะใช้ $\\pi_\\theta(a|s)$ (เช่น softmax, categorical) ไม่ใช่ argmax ซึ่งเป็น deterministic policy ยกตัวอย่างเช่น $\\pi_\\theta(a=0|s) = 0.4$, $\\pi_\\theta(a=1|s) = 0.6$ เราจะทำการ sample action ตามความน่าจะเป็นเหล่านี้ ไม่ใช่ argmax ซึ่งจะได้ action 1\n",
    "\n",
    "    - Observation space เป็น Continuous ส่วน Action space เป็น Discrete หรือ continuous ก็ได้ ขึ้นอยู่กับรูปแบบ policy ที่เลือก ถ้าเป็น discrete ต้องใช้ softmax กับ linear output แต่หากเป็น continuous ให้ใช้ Gaussian กับ linear output แทน\n",
    "\n",
    "    - Exploration เกิดจาก stochastic policy โดยตรง เพราะ agent มีโอกาสสุ่ม action ทุกครั้ง ถ้า policy เรียนรู้แล้วว่า action หนึ่งได้ reward มาก แล้ว probability ของ action นั้นจะสูงขึ้นเองซึ่งจะกลายเป็น exploitation โดยอัตโนมัติ\n",
    "\n",
    "- **MC-REINFORCE ใช้สูตรของ Monte Carlo Policy Gradient**\n",
    "    $$\n",
    "    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t \\right]\n",
    "    $$\n",
    "\n",
    "\n",
    "    โดยที่:\n",
    "\n",
    "    - $\\pi_\\theta(a_t | s_t)$ คือ policy (เช่น softmax over linear output หรือ neural net)\n",
    "\n",
    "    - $G_t$ คือ return รวม ตั้งแต่ timestep $t$ ถึงจบ episode\n",
    "\n",
    "    - เราเก็บทั้ง trajectory จนจบ (Monte Carlo) แล้วค่อยอัปเดต\n",
    "\n",
    "🧠 ขั้นตอนโดยรวม:\n",
    "\n",
    "- สุ่ม trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)$ โดย sample จาก policy $\\pi_\\theta$\n",
    "\n",
    "- คำนวณ return $G_t$ จาก timestep $t$ ถึงสิ้นสุด episode\n",
    "\n",
    "- คำนวณ gradient: $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "- ปรับน้ำหนักของ policy ด้วย `gradient ascent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of MC-REINFORCE in CartPole**\n",
    "\n",
    "ใช้ policy network แบบ softmax:$π_θ(a∣s)=softmax(Ws)$\n",
    "\n",
    "สมมุติ policy network เริ่มต้นแบบง่าย ๆ:\n",
    "\n",
    "- ใช้ weight $W \\in \\mathbb{R}^{2 \\times 4}$ → 2 action × 4 state dim\n",
    "\n",
    "- state ขณะนั้น: $s_0 = [0.1, 0.0, 0.05, -0.02]$\n",
    "\n",
    "✅ 1. สุ่ม trajectory จาก policy\n",
    "\n",
    "สมมุติว่า agent สุ่มได้ trajectory นี้:\n",
    "\n",
    "| t | State $s_t$ | Action $a_t$ | Reward $r_{t+1}$ | $\\pi_\\theta(a_t \\mid s_t)$ |\n",
    "|---|-------------|---------------|-------------------|-----------------------------|\n",
    "| 0 | $s_0$       | 1             | 1                 | 0.6                         |\n",
    "| 1 | $s_1$       | 0             | 1                 | 0.4                         |\n",
    "| 2 | $s_2$       | 1             | 1                 | 0.7                         |\n",
    "| 3 | $s_3$       | 1             | 1                 | 0.8                         |\n",
    "\n",
    "✅ Episode จบที่ timestep 4 → ได้ reward = 1 ทุก timestep\n",
    "\n",
    "✅ 2. คำนวณ Return $G_t$\n",
    "\n",
    "ให้ $\\gamma = 0.99$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_3 &= r_4 = 1 \\\\\n",
    "G_2 &= r_3 + \\gamma G_3 = 1 + 0.99 \\cdot 1 = 1.99 \\\\\n",
    "G_1 &= r_2 + \\gamma G_2 = 1 + 0.99 \\cdot 1.99 = 2.9701 \\\\\n",
    "G_0 &= r_1 + \\gamma G_1 = 1 + 0.99 \\cdot 2.9701 = 3.9404\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "|Time t\t|$G_t$|\n",
    "|-------|---------|\n",
    "0|\t3.9404|\n",
    "1|\t2.9701|\n",
    "2|\t1.99|\n",
    "3|\t1.0|\n",
    "\n",
    "✅ 3. คำนวณ gradient จากแต่ละ step\n",
    "\n",
    "เราจะใช้:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t\n",
    "$$\n",
    "\n",
    "- ตัวอย่าง Step 0:\n",
    "\n",
    "    - สมมุติว่า policy network ทำนาย softmax:\n",
    "\n",
    "    $$\n",
    "    \\pi(a = 0 \\mid s_0) = 0.4,\\quad \\pi(a = 1 \\mid s_0) = 0.6\n",
    "    $$\n",
    "\n",
    "    - เลือก $a_0 = 1$\n",
    "\n",
    "    - ได้:\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta \\log \\pi_\\theta(1 \\mid s_0) = \\nabla_\\theta \\log(0.6)\n",
    "    $$\n",
    "\n",
    "    - คูณกับ return:\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta J \\leftarrow \\nabla_\\theta \\log(0.6) \\cdot 3.9404\n",
    "    $$\n",
    "\n",
    "- Step 1:\n",
    "\n",
    "    - $\\log \\pi(0 \\mid s_1) = \\log(0.4) \\approx -0.9163$\n",
    "\n",
    "    - $-(-0.9163) \\cdot 2.9701 = 2.722$\n",
    "\n",
    "- Step 2:\n",
    "\n",
    "    - $\\log \\pi(1 \\mid s_2) = \\log(0.7) \\approx -0.3567$\n",
    "\n",
    "    - $-(-0.3567) \\cdot 1.9900 = 0.709$\n",
    "\n",
    "- Step 3:\n",
    "\n",
    "    - $\\log \\pi(1 \\mid s_3) = \\log(0.8) \\approx -0.2231$\n",
    "\n",
    "    - $-(-0.2231) \\cdot 1.0000 = 0.223$\n",
    "\n",
    "- รวม Loss ทั้งหมด Total Loss=2.013+2.722+0.709+0.223=5.667\n",
    "\n",
    "📌 สรุป\n",
    "\n",
    "| t   | $\\log \\pi(a_t \\mid s_t)$ | $G_t$   | $-\\log \\pi \\cdot G_t$ |\n",
    "|-----|---------------------------|---------|-------------------------|\n",
    "| 0   | -0.5108                   | 3.9404  | 2.013                   |\n",
    "| 1   | -0.9163                   | 2.9701  | 2.722                   |\n",
    "| 2   | -0.3567                   | 1.9900  | 0.709                   |\n",
    "| 3   | -0.2231                   | 1.0000  | 0.223                   |\n",
    "|     |                           |         | **Total: 5.667**        |\n",
    "\n",
    "**4. Do Gradient Ascent**\n",
    "\n",
    "- Call `loss.backward()` to compute gradient\n",
    "\n",
    "- Call `optimizer.step()` to update policy network weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Deterministic Policy Gradient (DDPG)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Advantage Actor-Critic (A2C)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Proximal Policy Optimization (PPO)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ แนวคิดหลักของ PPO\n",
    "\n",
    "PPO คือหนึ่งใน Policy Gradient algorithms ที่ได้รับความนิยมสูงมาก ซึ่งพัฒนาโดย OpenAI โดยมีเป้าหมายหลักเพื่อ:\n",
    "\n",
    "- ทำให้การอัปเดตนโยบาย (Policy) มีเสถียรภาพ (stable)\n",
    "- เรียนรู้จากข้อมูลที่มาจาก policy ปัจจุบัน (on-policy)\n",
    "- หลีกเลี่ยงการอัปเดตนโยบายแบบ “แรงเกินไป” ซึ่งอาจทำให้ performance แย่ลง\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗 โครงสร้าง PPO\n",
    "\n",
    "PPO ใช้แนวทาง **Actor-Critic** ซึ่งประกอบด้วย:\n",
    "\n",
    "- **Actor**: เรียนรู้นโยบาย $\\pi_\\theta(a|s)$ → ใช้เลือก action\n",
    "- **Critic**: ประเมิน value ของ state หรือ action เช่น $V(s)$ หรือ $Q(s,a)$ → ใช้คำนวณ advantage\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 หลักการทำงานของ PPO (Step-by-Step)\n",
    "\n",
    "### 1. **Collect Trajectories**\n",
    "- ให้ agent วิ่งใน environment ตาม policy ปัจจุบัน\n",
    "- เก็บข้อมูล: $(s_t, a_t, r_t, \\log \\pi(a_t|s_t), done)$\n",
    "- รอจนได้ rollout ครบ (เช่น 2048 steps หรือ 1 episode)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Compute Returns & Advantages**\n",
    "- คำนวณ **Monte Carlo return** หรือใช้ **GAE (Generalized Advantage Estimation)**:\n",
    "  \n",
    "  ```math\n",
    "  A_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + ... ≈ R_t - V(s_t)\n",
    "\n",
    "### 3. Surrogate Objective with Clipping\n",
    "\n",
    "- PPO ใช้ surrogate loss function เพื่อควบคุมการอัปเดตของนโยบาย:\n",
    "- rt(θ)=πθ(at∣st)πθold(at∣st)\n",
    "- LCLIP(θ)=Et[min⁡(rt(θ)At,clip(rt(θ),1−ϵ,1+ϵ)At)]\n",
    "- ถ้า $r_t$ เบี่ยงเบนจาก 1 มากเกินไป → จะถูก clip ไว้ เพื่อป้องกัน policy เปลี่ยนแปลงเร็วเกินไป\n",
    "\n",
    "### 4. Update Policy and Value Function\n",
    "\n",
    "- อัปเดต Actor ด้วย loss จาก surrogate objective\n",
    "\n",
    "- อัปเดต Critic ด้วย MSE loss ระหว่าง $V(s_t)$ กับ return\n",
    "\n",
    "### 5. Repeat Training for Multiple Epochs\n",
    "\n",
    "- ใช้ข้อมูล rollout เดิมฝึกได้หลายรอบ (เช่น 4-10 epochs)\n",
    "\n",
    "- ทำให้ sample efficient โดยไม่ต้องใช้ replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Soft Actor-Critic (SAC)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 2: Setting up Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous homework, you will implement a common components that will be the same in most of the function approximation-based RL in the RL_base_function.py.The core components should include, but are not limited to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**1. RL Base class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - Number of actions: The total number of discrete actions available to the agent.\n",
    "\n",
    "        - Action range: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "        - Discretize state weight: Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "        - Learning rate: Determines how quickly the model updates based on new information.\n",
    "\n",
    "        - Initial epsilon: The starting probability of taking a random action in an ε-greedy policy.\n",
    "\n",
    "        - Epsilon decay rate: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "        - Final epsilon: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "        - Discount factor: A coefficient (γ) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "        - Buffer size: Maximum number of experiences the buffer can hold.\n",
    "\n",
    "        - Batch size: Number of experiences to sample per batch.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - scale_action(): scale the action (if it is computed from the sigmoid or softmax function) to the proper length.\n",
    "\n",
    "        - decay_epsilon(): Decreases epsilon over time and returns the updated value.\n",
    "\n",
    "- Additional details about these functions are provided in the class file. You may also implement additional functions for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**scale_action()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_action(self, action):\n",
    "    \"\"\"\n",
    "    Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "    Args:\n",
    "        action (int): Discrete action in range [0, n].\n",
    "        n (int): Number of discrete actions (inclusive range from 0 to n).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled action tensor.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Unpack the minimum and maximum values of the action range\n",
    "    action_min, action_max = self.action_range\n",
    "\n",
    "    # Scale the discrete action index (0 to num_of_action-1) to a continuous value within [action_min, action_max]\n",
    "    scaled = action_min + (action / (self.num_of_action - 1)) * (action_max - action_min)\n",
    "\n",
    "    # Check if the scaled value is already a torch.Tensor\n",
    "    if isinstance(scaled, torch.Tensor):\n",
    "        # If yes, detach it from any computation graph and convert to float32\n",
    "        return scaled.clone().detach().to(dtype=torch.float32)\n",
    "    else:\n",
    "        # Otherwise, convert it into a torch.Tensor of type float32\n",
    "        return torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**decay_epsilon()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self):\n",
    "    \"\"\"\n",
    "    Decay epsilon value to reduce exploration over time.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Decay the exploration rate (epsilon) by multiplying with epsilon_decay,\n",
    "    # but ensure it doesn't go below the minimum value (final_epsilon)\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**2. Replay Buffer Class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- A class use to store state, action, reward, next state, and termination status from each timestep in episode to use as a dataset to train neural networks. This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - memory: FIFO buffer to store the trajectory within a certain time window.\n",
    "\n",
    "        - batch_size: Number of data samples drawn from memory to train the neural network.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - add(): Add state, action, reward, next state, and termination status to the FIFO buffer. Discard the oldest data in the buffer\n",
    "\n",
    "        - sample(): Sample data from memory to use in the neural network training.\n",
    "\n",
    "    - <font color=\"orange\">**Note that some algorithms may not use all of the data mentioned above to train the neural network.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**add()**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**sample()**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**3. Algorithm folder**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This folder should include:\n",
    "\n",
    "    - Linear Q Learning class\n",
    "\n",
    "    - Deep Q-Network class\n",
    "\n",
    "    - REINFORCE Class\n",
    "\n",
    "    - One class chosen from the Part 1.\n",
    "\n",
    "- Each class should inherit from the RL Base class in RL_base_function.py and include:\n",
    "\n",
    "    - A constructor which initializes the same variables as the class it inherits from.\n",
    "\n",
    "    - Superclass Initialization (super().__init__()).\n",
    "\n",
    "    - An update() function that updates the agent’s learnable parameters and advances the training step.\n",
    "\n",
    "    - A select_action() function select the action according to current policy.\n",
    "\n",
    "    - A learn() function that train the regression or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Linear Q-Learning class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Deep Q-Network (DQN) class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**REINFORCE class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**DDPG class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**A2C class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**PPO class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**SAC class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 3: Trainning & Playing to stabilize Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the training loop in train script and main() in the play script (in the \"Can be modified\" area of both files). Additionally, you must collect data, analyze results, and save models for evaluating agent performance.\n",
    "\n",
    "- Training the Agent\n",
    "\n",
    "    - Stabilizing Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task SwingUp-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "- Playing\n",
    "\n",
    "    - Stabilize Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ``` \n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task SwingUp-Isaac-Cartpole-v0 \n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**train.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**play.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 4: Evaluate Cart-Pole Agent performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must evaluate the agent's performance in terms of learning efficiency (i.e., how well the agent learns to receive higher rewards) and deployment performance (i.e., how well the agent performs in the Cart-Pole problem). Analyze and visualize the results to determine:\n",
    "\n",
    "- Which algorithm performs best?\n",
    "\n",
    "- Why does it perform better than the others?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of deployment performance**</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
