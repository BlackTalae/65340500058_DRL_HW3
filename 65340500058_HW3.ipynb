{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3: Function-based RL**\n",
    "#### **Created by 65340500058 Anuwit Intet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives:**\n",
    "\n",
    "- Understand how function approximation works and how to implement it.\n",
    "\n",
    "- Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "- Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "- Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "- Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 1: Understanding the Algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you have to implement 4 different function approximation-based RL algorithms:\n",
    "\n",
    "- Linear Q-Learning\n",
    " \n",
    "- Deep Q-Network (DQN)\n",
    "\n",
    "- REINFORCE algorithm\n",
    "\n",
    "- One algorithm chosen from the following Actor-Critic methods:\n",
    "\n",
    "    - Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "    - Advantage Actor-Critic (A2C)\n",
    "\n",
    "    - Proximal Policy Optimization (PPO)\n",
    "    \n",
    "    - Soft Actor-Critic (SAC)\n",
    "\n",
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it follows a value-based, policy-based, or Actor-Critic approach\n",
    "\n",
    "- the type of policy it learns (stochastic or deterministic)\n",
    "\n",
    "- the type of observation space and action space (discrete or continuous)\n",
    "\n",
    "- how each advanced RL method balances exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Linear Q-Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Linear Q-Learning**\n",
    "\n",
    "  - Linear Q-Learning is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "  - This algorithm uses the deterministic policy because Linear Q-Learning uses a Œµ-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "  - Linear Q-Learning is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max‚Å°_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "  - To balance Exploration vs Exploitation, Linear Q-Learning uses a Œµ-greedy policy, i.e. random action with probability Œµ and greedy action with probability 1‚àíŒµ.\n",
    "\n",
    "- **In Linear Q-Learning, Q-Function is estimate by**\n",
    "\n",
    "  $$\n",
    "  Q(s,a) = \\phi(s,a)^T w\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  - $\\phi(s,a)$ is feature vector of state-action pair  \n",
    "  - $w$ is weight vector\n",
    "\n",
    "- **And update weight by this,**\n",
    "\n",
    "  $$\n",
    "  w \\leftarrow w + \\alpha \\cdot \\delta \\cdot \\phi(s, a)\n",
    "  $$\n",
    "\n",
    "  where: \n",
    "  - $\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$ is TD error\n",
    "  - $\\alpha$ is learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of Linear Q-Learning in CartPole**\n",
    "\n",
    "- **State**: Vector of size 4:  \n",
    "  $$\n",
    "  s = [x, \\dot{x}, \\theta, \\dot{\\theta}]\n",
    "  $$\n",
    "- **Action space**: 2 Actions (discrete):\n",
    "\n",
    "  - `0` = push cart to left\n",
    "\n",
    "  - `1` = push cart to right\n",
    "\n",
    "---\n",
    "\n",
    "We will approximate $Q(s, a)$ with a linear function like this:\n",
    "\n",
    "$$\n",
    "Q(s, a) = w_a^T s\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_0$, $w_1$ are weight vectors for action 0 and 1 respectively\n",
    "- or combined into a matrix $W \\in \\mathbb{R}^{2 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "- $s = [0.0, 0.5, 0.05, -0.2]$\n",
    "- $W = \\begin{bmatrix} 0.1 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.1 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- Choose action $a = 1$\n",
    "- Acheive reward $r = 1$\n",
    "- next state: $s' = [0.01, 0.55, 0.045, -0.18]$\n",
    "- $\\alpha = 0.1$, $\\gamma = 0.99$\n",
    "\n",
    "\n",
    "**1. Calculate $Q(s, a)$**\n",
    "\n",
    "$$\n",
    "Q(s, a=1) = w_1^T s = 0.0*0.0 + 0.1*0.5 + 0.0*0.05 + 0.0*(-0.2) = 0.05\n",
    "$$\n",
    "\n",
    "**2. Calculate $\\max_{a'} Q(s', a')$**\n",
    "\n",
    "$$\n",
    "Q(s', 0) = w_0^T s' = 0.1*0.01 = 0.001 \\\\\n",
    "Q(s', 1) = w_1^T s' = 0.1*0.55 = 0.055 \\\\\n",
    "\\Rightarrow \\max_{a'} Q(s', a') = 0.055\n",
    "$$\n",
    "\n",
    "**3. Calculate TD Error**\n",
    "\n",
    "$$\n",
    "\\delta = r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\\\\n",
    "= 1 + 0.99 \\cdot 0.055 - 0.05 = 1.00445\n",
    "$$\n",
    "\n",
    "**4. Update weight**\n",
    "\n",
    "$w_1$ only:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 + \\alpha \\cdot \\delta \\cdot s \\\\\n",
    "= [0.0, 0.1, 0.0, 0.0] + 0.1 \\cdot 1.00445 \\cdot [0.0, 0.5, 0.05, -0.2] \\\\\n",
    "= [0.0, 0.1502, 0.005, -0.0201]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Q-Network (DQN)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Deep Q-Network**\n",
    "\n",
    "    - Deep Q-Network has the same property of Linear Q-Learning, Both has difference in term of complexity of network that from 1 layer neural network to deep neural network (more than 1 layer).\n",
    "\n",
    "    - Deep Q-Network is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "    - This algorithm uses the deterministic policy because Deep Q-Network uses a Œµ-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "    - Deep Q-Network is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max‚Å°_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "    - To balance Exploration vs Exploitation, Deep Q-Network uses a Œµ-greedy policy, i.e. random action with probability Œµ and greedy action with probability 1‚àíŒµ.\n",
    "\n",
    "- DQN solves this problem by using a deep neural network to replace the Q-table with an approximate function: $Q(s,a;Œ∏)$.\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - Œ∏ is the neural network parameter\n",
    "\n",
    "    - input = state s\n",
    "\n",
    "    - output = Q value for every action\n",
    "\n",
    "- DQN training consists of 2 main techniques:\n",
    "\n",
    "    - Experience Replay\n",
    "        - Store the experience $(s, a, r, s', done)$ in a buffer\n",
    "        - Then randomly select a mini-batch to train it\n",
    "\n",
    "    - Target Network\n",
    "        - Use a separate network called target network‚Äã to calculate the target:$y=r+Œ≥max_{‚Å°a‚Ä≤}Q_{target}(s‚Ä≤,a‚Ä≤)$\n",
    "        - Then update only the main network Q periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of DQN in CartPole**\n",
    "\n",
    "**1. Choose action with $\\epsilon$-greedy**\n",
    "\n",
    "- Suppose $\\epsilon = 0.1$ ‚Üí Random chance = 10%\n",
    "\n",
    "- Luckily pick Random ‚Üí Use Q-network to predict:\n",
    "$$\n",
    "Q(s, a=0) = 0.4,\\quad Q(s, a=1) = 0.6\n",
    "$$\n",
    "\n",
    "- Choose **action = 1** (right) because Q is highest\n",
    "\n",
    "**2. Send action to environment**\n",
    "\n",
    "- Got:\n",
    "\n",
    "  - reward = 1\n",
    "\n",
    "  - next_state = [0.06, 0.025, -0.015, 0.035]\n",
    "\n",
    "  - done = False (Not yet failed)\n",
    "\n",
    "**3. Store transition**\n",
    "\n",
    "- Store $(s, a=1, r=1, s', done=False)$ in the replay buffer.\n",
    "\n",
    "**4. Assume that the buffer is sufficient ‚Üí Start training 1 round**\n",
    "\n",
    "- Suppose that we store the transition in the replay buffer and randomly get 2 mini-batch samples as follows:\n",
    "\n",
    "| Index | State (s)                  | Action (a) | Reward (r) | Next State (s‚Ä≤)             | Done  |\n",
    "|-------|----------------------------|------------|------------|------------------------------|--------|\n",
    "| 0     | [0.05, 0.02, -0.01, 0.03]  | 1          | 1.0        | [0.06, 0.025, -0.015, 0.035] | False  |\n",
    "| 1     | [-0.01, -0.03, 0.02, -0.02] | 0         | 1.0        | [0.00, -0.02, 0.01, -0.01]   | True   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Calculate Q(s, a) from policy network**\n",
    "\n",
    "Suppose the policy network gives:\n",
    "\n",
    "| Index | Q(s, a=0) | Q(s, a=1) |\n",
    "|-------|-----------|-----------|\n",
    "| 0 | 0.5 | 0.65 |\n",
    "| 1 | 0.6 | 0.55 |\n",
    "\n",
    "Get Q of the selected action:\n",
    "\n",
    "- Example 0: action = 1 ‚Üí Q = 0.65\n",
    "\n",
    "- Example 1: action = 0 ‚Üí Q = 0.6\n",
    "\n",
    "**Q(s,a) = [0.65, 0.6]**\n",
    "\n",
    "**Calculate Target Q(s‚Ä≤, a‚Ä≤) from target network**\n",
    "\n",
    "Suppose Target Network gives:\n",
    "\n",
    "| Index | Q(s‚Ä≤, a=0) | Q(s‚Ä≤, a=1) | Done |\n",
    "|-------|------------|------------|--------|\n",
    "| 0 | 0.4 | 0.6 | False |\n",
    "| 1 | -- | -- | True |\n",
    "\n",
    "‚Üí max Q(s‚Ä≤) only for not done:\n",
    "\n",
    "- Index 0: max = 0.6 \n",
    "\n",
    "- Index 1: terminal ‚Üí max = 0\n",
    "\n",
    "**max_next_q_values ‚Äã‚Äã= [0.6, 0.0]**\n",
    "\n",
    "**7. Calculate Target Q-value (Bellman target)**\n",
    "\n",
    "Use the formula:\n",
    "$$\n",
    "y = r + \\gamma \\cdot (1 - \\text{done}) \\cdot \\max Q(s', a')\n",
    "$$\n",
    "\n",
    "‡πÉ‡∏´‡πâ $\\gamma = 0.99$:\n",
    "\n",
    "- Index 0:  $y = 1.0 + 0.99 \\cdot 0.6 = 1.594$\n",
    "\n",
    "- Index 1:  $y = 1.0 + 0 = 1.0$\n",
    "\n",
    "**Target Q-values = [1.594, 1.0]**\n",
    "\n",
    "**8. Calculate Loss**\n",
    "\n",
    "Use the formula:\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\sum_i (Q(s_i, a_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "- $Q(s_i, a_i)$ is Q-value from policy network\n",
    "\n",
    "- $y_i$ is Q-value from target network\n",
    "\n",
    "Substitute the value:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\left[ (0.65 - 1.594)^2 + (0.6 - 1.0)^2 \\right] \\\\\n",
    "= \\frac{1}{2} \\left[ 0.891 + 0.16 \\right] = \\frac{1.051}{2} = \\mathbf{0.5255}\n",
    "$$\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Index | Q(s, a) | Target y | Loss per item |\n",
    "|-------|---------|----------|----------------|\n",
    "| 0     | 0.65    | 1.594    | 0.891          |\n",
    "| 1     | 0.6     | 1.0      | 0.160          |\n",
    "|       |         |          | Total = 1.051 |\n",
    "|       |         |          | Avg = 0.5255  |\n",
    "\n",
    "**Final Loss = 0.5255**\n",
    "\n",
    "**9. Do Gradient Descent**\n",
    "\n",
    "- Call `loss.backward()` to compute gradient\n",
    "\n",
    "- Call `optimizer.step()` to update policy network weights\n",
    "\n",
    "**This is where Q-network learns from the latest experience that is randomly selected from the replay buffer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**REINFORCE algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About MC-REINFORCE Algorithm**\n",
    "\n",
    "    - MC-REINFORCE (Monte Carlo REINFORCE) ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Policy Gradient Method ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ (policy) ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡πà‡∏≤ Q-function ‡πÅ‡∏ö‡∏ö Q-learning\n",
    "\n",
    "    - MC-REINFORCE ‡πÄ‡∏õ‡πá‡∏ô Policy-based ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ REINFORCE ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Q-value ‡∏´‡∏£‡∏∑‡∏≠ V-value ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ $\\pi_\\theta$ ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (pure policy gradient method)\n",
    "\n",
    "    - type of policy is Stochastic policy ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ $\\pi_\\theta(a|s)$ (‡πÄ‡∏ä‡πà‡∏ô softmax, categorical) ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà argmax ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô deterministic policy ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô $\\pi_\\theta(a=0|s) = 0.4$, $\\pi_\\theta(a=1|s) = 0.6$ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ sample action ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà argmax ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÑ‡∏î‡πâ action 1\n",
    "\n",
    "    - Observation space ‡πÄ‡∏õ‡πá‡∏ô Continuous ‡∏™‡πà‡∏ß‡∏ô Action space ‡πÄ‡∏õ‡πá‡∏ô Discrete ‡∏´‡∏£‡∏∑‡∏≠ continuous ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö policy ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô discrete ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ softmax ‡∏Å‡∏±‡∏ö linear output ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô continuous ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Gaussian ‡∏Å‡∏±‡∏ö linear output ‡πÅ‡∏ó‡∏ô\n",
    "\n",
    "    - Exploration ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å stochastic policy ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ agent ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏∏‡πà‡∏° action ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏ñ‡πâ‡∏≤ policy ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤ action ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏î‡πâ reward ‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡πâ‡∏ß probability ‡∏Ç‡∏≠‡∏á action ‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏≠‡∏á‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô exploitation ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "\n",
    "- **MC-REINFORCE ‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£‡∏Ç‡∏≠‡∏á Monte Carlo Policy Gradient**\n",
    "    $$\n",
    "    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t \\right]\n",
    "    $$\n",
    "\n",
    "\n",
    "    ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:\n",
    "\n",
    "    - $\\pi_\\theta(a_t | s_t)$ ‡∏Ñ‡∏∑‡∏≠ policy (‡πÄ‡∏ä‡πà‡∏ô softmax over linear output ‡∏´‡∏£‡∏∑‡∏≠ neural net)\n",
    "\n",
    "    - $G_t$ ‡∏Ñ‡∏∑‡∏≠ return ‡∏£‡∏ß‡∏° ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà timestep $t$ ‡∏ñ‡∏∂‡∏á‡∏à‡∏ö episode\n",
    "\n",
    "    - ‡πÄ‡∏£‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á trajectory ‡∏à‡∏ô‡∏à‡∏ö (Monte Carlo) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï\n",
    "\n",
    "üß† ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°:\n",
    "\n",
    "- ‡∏™‡∏∏‡πà‡∏° trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)$ ‡πÇ‡∏î‡∏¢ sample ‡∏à‡∏≤‡∏Å policy $\\pi_\\theta$\n",
    "\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì return $G_t$ ‡∏à‡∏≤‡∏Å timestep $t$ ‡∏ñ‡∏∂‡∏á‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î episode\n",
    "\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient: $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "- ‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á policy ‡∏î‡πâ‡∏ß‡∏¢ `gradient ascent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of MC-REINFORCE in CartPole**\n",
    "\n",
    "‡πÉ‡∏ä‡πâ policy network ‡πÅ‡∏ö‡∏ö softmax:$œÄ_Œ∏(a‚à£s)=softmax(Ws)$\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥ policy network ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ:\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ weight $W \\in \\mathbb{R}^{2 \\times 4}$ ‚Üí 2 action √ó 4 state dim\n",
    "\n",
    "- state ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏±‡πâ‡∏ô: $s_0 = [0.1, 0.0, 0.05, -0.02]$\n",
    "\n",
    "‚úÖ 1. ‡∏™‡∏∏‡πà‡∏° trajectory ‡∏à‡∏≤‡∏Å policy\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤ agent ‡∏™‡∏∏‡πà‡∏°‡πÑ‡∏î‡πâ trajectory ‡∏ô‡∏µ‡πâ:\n",
    "\n",
    "| t | State $s_t$ | Action $a_t$ | Reward $r_{t+1}$ | $\\pi_\\theta(a_t \\mid s_t)$ |\n",
    "|---|-------------|---------------|-------------------|-----------------------------|\n",
    "| 0 | $s_0$       | 1             | 1                 | 0.6                         |\n",
    "| 1 | $s_1$       | 0             | 1                 | 0.4                         |\n",
    "| 2 | $s_2$       | 1             | 1                 | 0.7                         |\n",
    "| 3 | $s_3$       | 1             | 1                 | 0.8                         |\n",
    "\n",
    "‚úÖ Episode ‡∏à‡∏ö‡∏ó‡∏µ‡πà timestep 4 ‚Üí ‡πÑ‡∏î‡πâ reward = 1 ‡∏ó‡∏∏‡∏Å timestep\n",
    "\n",
    "‚úÖ 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Return $G_t$\n",
    "\n",
    "‡πÉ‡∏´‡πâ $\\gamma = 0.99$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_3 &= r_4 = 1 \\\\\n",
    "G_2 &= r_3 + \\gamma G_3 = 1 + 0.99 \\cdot 1 = 1.99 \\\\\n",
    "G_1 &= r_2 + \\gamma G_2 = 1 + 0.99 \\cdot 1.99 = 2.9701 \\\\\n",
    "G_0 &= r_1 + \\gamma G_1 = 1 + 0.99 \\cdot 2.9701 = 3.9404\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "|Time t\t|$G_t$|\n",
    "|-------|---------|\n",
    "0|\t3.9404|\n",
    "1|\t2.9701|\n",
    "2|\t1.99|\n",
    "3|\t1.0|\n",
    "\n",
    "‚úÖ 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient ‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ step\n",
    "\n",
    "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t\n",
    "$$\n",
    "\n",
    "- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Step 0:\n",
    "\n",
    "    - ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤ policy network ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ softmax:\n",
    "\n",
    "    $$\n",
    "    \\pi(a = 0 \\mid s_0) = 0.4,\\quad \\pi(a = 1 \\mid s_0) = 0.6\n",
    "    $$\n",
    "\n",
    "    - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å $a_0 = 1$\n",
    "\n",
    "    - ‡πÑ‡∏î‡πâ:\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta \\log \\pi_\\theta(1 \\mid s_0) = \\nabla_\\theta \\log(0.6)\n",
    "    $$\n",
    "\n",
    "    - ‡∏Ñ‡∏π‡∏ì‡∏Å‡∏±‡∏ö return:\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta J \\leftarrow \\nabla_\\theta \\log(0.6) \\cdot 3.9404\n",
    "    $$\n",
    "\n",
    "- Step 1:\n",
    "\n",
    "    - $\\log \\pi(0 \\mid s_1) = \\log(0.4) \\approx -0.9163$\n",
    "\n",
    "    - $-(-0.9163) \\cdot 2.9701 = 2.722$\n",
    "\n",
    "- Step 2:\n",
    "\n",
    "    - $\\log \\pi(1 \\mid s_2) = \\log(0.7) \\approx -0.3567$\n",
    "\n",
    "    - $-(-0.3567) \\cdot 1.9900 = 0.709$\n",
    "\n",
    "- Step 3:\n",
    "\n",
    "    - $\\log \\pi(1 \\mid s_3) = \\log(0.8) \\approx -0.2231$\n",
    "\n",
    "    - $-(-0.2231) \\cdot 1.0000 = 0.223$\n",
    "\n",
    "- ‡∏£‡∏ß‡∏° Loss ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î Total Loss=2.013+2.722+0.709+0.223=5.667\n",
    "\n",
    "üìå ‡∏™‡∏£‡∏∏‡∏õ\n",
    "\n",
    "| t   | $\\log \\pi(a_t \\mid s_t)$ | $G_t$   | $-\\log \\pi \\cdot G_t$ |\n",
    "|-----|---------------------------|---------|-------------------------|\n",
    "| 0   | -0.5108                   | 3.9404  | 2.013                   |\n",
    "| 1   | -0.9163                   | 2.9701  | 2.722                   |\n",
    "| 2   | -0.3567                   | 1.9900  | 0.709                   |\n",
    "| 3   | -0.2231                   | 1.0000  | 0.223                   |\n",
    "|     |                           |         | **Total: 5.667**        |\n",
    "\n",
    "**4. Do Gradient Ascent**\n",
    "\n",
    "- Call `loss.backward()` to compute gradient\n",
    "\n",
    "- Call `optimizer.step()` to update policy network weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Deterministic Policy Gradient (DDPG)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Advantage Actor-Critic (A2C)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Proximal Policy Optimization (PPO)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á PPO\n",
    "\n",
    "PPO ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô Policy Gradient algorithms ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏î‡∏¢ OpenAI ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠:\n",
    "\n",
    "- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ (Policy) ‡∏°‡∏µ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏†‡∏≤‡∏û (stable)\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å policy ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (on-policy)\n",
    "- ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö ‚Äú‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‚Äù ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ performance ‡πÅ‡∏¢‡πà‡∏•‡∏á\n",
    "\n",
    "---\n",
    "\n",
    "## üèó ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á PPO\n",
    "\n",
    "PPO ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á **Actor-Critic** ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
    "\n",
    "- **Actor**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ $\\pi_\\theta(a|s)$ ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action\n",
    "- **Critic**: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô value ‡∏Ç‡∏≠‡∏á state ‡∏´‡∏£‡∏∑‡∏≠ action ‡πÄ‡∏ä‡πà‡∏ô $V(s)$ ‡∏´‡∏£‡∏∑‡∏≠ $Q(s,a)$ ‚Üí ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì advantage\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á PPO (Step-by-Step)\n",
    "\n",
    "### 1. **Collect Trajectories**\n",
    "- ‡πÉ‡∏´‡πâ agent ‡∏ß‡∏¥‡πà‡∏á‡πÉ‡∏ô environment ‡∏ï‡∏≤‡∏° policy ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n",
    "- ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: $(s_t, a_t, r_t, \\log \\pi(a_t|s_t), done)$\n",
    "- ‡∏£‡∏≠‡∏à‡∏ô‡πÑ‡∏î‡πâ rollout ‡∏Ñ‡∏£‡∏ö (‡πÄ‡∏ä‡πà‡∏ô 2048 steps ‡∏´‡∏£‡∏∑‡∏≠ 1 episode)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Compute Returns & Advantages**\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Monte Carlo return** ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ **GAE (Generalized Advantage Estimation)**:\n",
    "  \n",
    "  ```math\n",
    "  A_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + ... ‚âà R_t - V(s_t)\n",
    "\n",
    "### 3. Surrogate Objective with Clipping\n",
    "\n",
    "- PPO ‡πÉ‡∏ä‡πâ surrogate loss function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡∏≠‡∏á‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢:\n",
    "- rt(Œ∏)=œÄŒ∏(at‚à£st)œÄŒ∏old(at‚à£st)\n",
    "- LCLIP(Œ∏)=Et[min‚Å°(rt(Œ∏)At,clip(rt(Œ∏),1‚àíœµ,1+œµ)At)]\n",
    "- ‡∏ñ‡πâ‡∏≤ $r_t$ ‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏à‡∏≤‡∏Å 1 ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å clip ‡πÑ‡∏ß‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô policy ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "### 4. Update Policy and Value Function\n",
    "\n",
    "- ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Actor ‡∏î‡πâ‡∏ß‡∏¢ loss ‡∏à‡∏≤‡∏Å surrogate objective\n",
    "\n",
    "- ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Critic ‡∏î‡πâ‡∏ß‡∏¢ MSE loss ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á $V(s_t)$ ‡∏Å‡∏±‡∏ö return\n",
    "\n",
    "### 5. Repeat Training for Multiple Epochs\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• rollout ‡πÄ‡∏î‡∏¥‡∏°‡∏ù‡∏∂‡∏Å‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö (‡πÄ‡∏ä‡πà‡∏ô 4-10 epochs)\n",
    "\n",
    "- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ sample efficient ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Soft Actor-Critic (SAC)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 2: Setting up Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous homework, you will implement a common components that will be the same in most of the function approximation-based RL in the RL_base_function.py.The core components should include, but are not limited to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**1. RL Base class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - Number of actions: The total number of discrete actions available to the agent.\n",
    "\n",
    "        - Action range: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "        - Discretize state weight: Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "        - Learning rate: Determines how quickly the model updates based on new information.\n",
    "\n",
    "        - Initial epsilon: The starting probability of taking a random action in an Œµ-greedy policy.\n",
    "\n",
    "        - Epsilon decay rate: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "        - Final epsilon: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "        - Discount factor: A coefficient (Œ≥) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "        - Buffer size: Maximum number of experiences the buffer can hold.\n",
    "\n",
    "        - Batch size: Number of experiences to sample per batch.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - scale_action(): scale the action (if it is computed from the sigmoid or softmax function) to the proper length.\n",
    "\n",
    "        - decay_epsilon(): Decreases epsilon over time and returns the updated value.\n",
    "\n",
    "- Additional details about these functions are provided in the class file. You may also implement additional functions for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**scale_action()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_action(self, action):\n",
    "    \"\"\"\n",
    "    Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "    Args:\n",
    "        action (int): Discrete action in range [0, n].\n",
    "        n (int): Number of discrete actions (inclusive range from 0 to n).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled action tensor.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Unpack the minimum and maximum values of the action range\n",
    "    action_min, action_max = self.action_range\n",
    "\n",
    "    # Scale the discrete action index (0 to num_of_action-1) to a continuous value within [action_min, action_max]\n",
    "    scaled = action_min + (action / (self.num_of_action - 1)) * (action_max - action_min)\n",
    "\n",
    "    # Check if the scaled value is already a torch.Tensor\n",
    "    if isinstance(scaled, torch.Tensor):\n",
    "        # If yes, detach it from any computation graph and convert to float32\n",
    "        return scaled.clone().detach().to(dtype=torch.float32)\n",
    "    else:\n",
    "        # Otherwise, convert it into a torch.Tensor of type float32\n",
    "        return torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**decay_epsilon()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self):\n",
    "    \"\"\"\n",
    "    Decay epsilon value to reduce exploration over time.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Decay the exploration rate (epsilon) by multiplying with epsilon_decay,\n",
    "    # but ensure it doesn't go below the minimum value (final_epsilon)\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**2. Replay Buffer Class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- A class use to store state, action, reward, next state, and termination status from each timestep in episode to use as a dataset to train neural networks. This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - memory: FIFO buffer to store the trajectory within a certain time window.\n",
    "\n",
    "        - batch_size: Number of data samples drawn from memory to train the neural network.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - add(): Add state, action, reward, next state, and termination status to the FIFO buffer. Discard the oldest data in the buffer\n",
    "\n",
    "        - sample(): Sample data from memory to use in the neural network training.\n",
    "\n",
    "    - <font color=\"orange\">**Note that some algorithms may not use all of the data mentioned above to train the neural network.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**add()**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**sample()**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**3. Algorithm folder**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This folder should include:\n",
    "\n",
    "    - Linear Q Learning class\n",
    "\n",
    "    - Deep Q-Network class\n",
    "\n",
    "    - REINFORCE Class\n",
    "\n",
    "    - One class chosen from the Part 1.\n",
    "\n",
    "- Each class should inherit from the RL Base class in RL_base_function.py and include:\n",
    "\n",
    "    - A constructor which initializes the same variables as the class it inherits from.\n",
    "\n",
    "    - Superclass Initialization (super().__init__()).\n",
    "\n",
    "    - An update() function that updates the agent‚Äôs learnable parameters and advances the training step.\n",
    "\n",
    "    - A select_action() function select the action according to current policy.\n",
    "\n",
    "    - A learn() function that train the regression or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Linear Q-Learning class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Deep Q-Network (DQN) class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**REINFORCE class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**DDPG class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**A2C class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**PPO class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**SAC class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 3: Trainning & Playing to stabilize Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the training loop in train script and main() in the play script (in the \"Can be modified\" area of both files). Additionally, you must collect data, analyze results, and save models for evaluating agent performance.\n",
    "\n",
    "- Training the Agent\n",
    "\n",
    "    - Stabilizing Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task SwingUp-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "- Playing\n",
    "\n",
    "    - Stabilize Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ``` \n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task SwingUp-Isaac-Cartpole-v0 \n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**train.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**play.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 4: Evaluate Cart-Pole Agent performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must evaluate the agent's performance in terms of learning efficiency (i.e., how well the agent learns to receive higher rewards) and deployment performance (i.e., how well the agent performs in the Cart-Pole problem). Analyze and visualize the results to determine:\n",
    "\n",
    "- Which algorithm performs best?\n",
    "\n",
    "- Why does it perform better than the others?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of deployment performance**</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
