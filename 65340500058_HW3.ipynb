{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3: Function-based RL**\n",
    "#### **Created by 65340500058 Anuwit Intet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives:**\n",
    "\n",
    "- Understand how function approximation works and how to implement it.\n",
    "\n",
    "- Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "- Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "- Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "- Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 1: Understanding the Algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you have to implement 4 different function approximation-based RL algorithms:\n",
    "\n",
    "- Linear Q-Learning\n",
    " \n",
    "- Deep Q-Network (DQN)\n",
    "\n",
    "- REINFORCE algorithm\n",
    "\n",
    "- One algorithm chosen from the following Actor-Critic methods:\n",
    "\n",
    "    - Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "    - Advantage Actor-Critic (A2C)\n",
    "\n",
    "    - Proximal Policy Optimization (PPO)\n",
    "    \n",
    "    - Soft Actor-Critic (SAC)\n",
    "\n",
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it follows a value-based, policy-based, or Actor-Critic approach\n",
    "\n",
    "- the type of policy it learns (stochastic or deterministic)\n",
    "\n",
    "- the type of observation space and action space (discrete or continuous)\n",
    "\n",
    "- how each advanced RL method balances exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Linear Q-Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Linear Q-Learning**\n",
    "\n",
    "  - Linear Q-Learning is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "  - This algorithm uses the deterministic policy because Linear Q-Learning uses a ε-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "  - Linear Q-Learning is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max⁡_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "  - To balance Exploration vs Exploitation, Linear Q-Learning uses a ε-greedy policy, i.e. random action with probability ε and greedy action with probability 1−ε.\n",
    "\n",
    "- **In Linear Q-Learning, Q-Function is estimate by**\n",
    "\n",
    "  $$\n",
    "  Q(s,a) = \\phi(s,a)^T w\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  - $\\phi(s,a)$ is feature vector of state-action pair  \n",
    "  - $w$ is weight vector\n",
    "\n",
    "- **And update weight by this,**\n",
    "\n",
    "  $$\n",
    "  w \\leftarrow w + \\alpha \\cdot \\delta \\cdot \\phi(s, a)\n",
    "  $$\n",
    "\n",
    "  where: \n",
    "  - $\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$ is TD error\n",
    "  - $\\alpha$ is learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of Linear Q-Learning in CartPole**\n",
    "\n",
    "- **State**: Vector of size 4:  \n",
    "  $$\n",
    "  s = [x, \\dot{x}, \\theta, \\dot{\\theta}]\n",
    "  $$\n",
    "- **Action space**: 2 Actions (discrete):\n",
    "\n",
    "  - `0` = push cart to left\n",
    "\n",
    "  - `1` = push cart to right\n",
    "\n",
    "---\n",
    "\n",
    "We will approximate $Q(s, a)$ with a linear function like this:\n",
    "\n",
    "$$\n",
    "Q(s, a) = w_a^T s\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_0$, $w_1$ are weight vectors for action 0 and 1 respectively\n",
    "- or combined into a matrix $W \\in \\mathbb{R}^{2 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "- $s = [0.0, 0.5, 0.05, -0.2]$\n",
    "- $W = \\begin{bmatrix} 0.1 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.1 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- Choose action $a = 1$\n",
    "- Acheive reward $r = 1$\n",
    "- next state: $s' = [0.01, 0.55, 0.045, -0.18]$\n",
    "- $\\alpha = 0.1$, $\\gamma = 0.99$\n",
    "\n",
    "\n",
    "**1. Calculate $Q(s, a)$**\n",
    "\n",
    "$$\n",
    "Q(s, a=1) = w_1^T s = 0.0*0.0 + 0.1*0.5 + 0.0*0.05 + 0.0*(-0.2) = 0.05\n",
    "$$\n",
    "\n",
    "**2. Calculate $\\max_{a'} Q(s', a')$**\n",
    "\n",
    "$$\n",
    "Q(s', 0) = w_0^T s' = 0.1*0.01 = 0.001 \\\\\n",
    "Q(s', 1) = w_1^T s' = 0.1*0.55 = 0.055 \\\\\n",
    "\\Rightarrow \\max_{a'} Q(s', a') = 0.055\n",
    "$$\n",
    "\n",
    "**3. Calculate TD Error**\n",
    "\n",
    "$$\n",
    "\\delta = r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\\\\n",
    "= 1 + 0.99 \\cdot 0.055 - 0.05 = 1.00445\n",
    "$$\n",
    "\n",
    "**4. Update weight**\n",
    "\n",
    "$w_1$ only:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 + \\alpha \\cdot \\delta \\cdot s \\\\\n",
    "= [0.0, 0.1, 0.0, 0.0] + 0.1 \\cdot 1.00445 \\cdot [0.0, 0.5, 0.05, -0.2] \\\\\n",
    "= [0.0, 0.1502, 0.005, -0.0201]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Deep Q-Network (DQN)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Deep Q-Network**\n",
    "\n",
    "    - Deep Q-Network has the same property of Linear Q-Learning, Both has difference in term of complexity of network that from 1 layer neural network to deep neural network (more than 1 layer).\n",
    "\n",
    "    - Deep Q-Network is a value-based approach. It sometimes learns a function Q(s, a) that is used to determine the method by selecting the maximum Q-value action.\n",
    "\n",
    "    - This algorithm uses the deterministic policy because Deep Q-Network uses a ε-greedy policy which argmax Q-value, not uses the probability.\n",
    "\n",
    "    - Deep Q-Network is applied to continuous observation space (because it uses input feature vectors). But the action space must be discrete because it must compute $max⁡_Q(s,a)$, which must look at all actions. \n",
    "\n",
    "    - To balance Exploration vs Exploitation, Deep Q-Network uses a ε-greedy policy, i.e. random action with probability ε and greedy action with probability 1−ε.\n",
    "\n",
    "- DQN solves this problem by using a deep neural network to replace the Q-table with an approximate function: $Q(s,a;θ)$.\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - θ is the neural network parameter\n",
    "\n",
    "    - input = state s\n",
    "\n",
    "    - output = Q value for every action\n",
    "\n",
    "- DQN training consists of 2 main techniques:\n",
    "\n",
    "    - Experience Replay\n",
    "        - Store the experience $(s, a, r, s', done)$ in a buffer\n",
    "        - Then randomly select a mini-batch to train it\n",
    "\n",
    "    - Target Network\n",
    "        - Use a separate network called target network​ to calculate the target:$y=r+γmax_{⁡a′}Q_{target}(s′,a′)$\n",
    "        - Then update only the main network Q periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of DQN in CartPole**\n",
    "\n",
    "**1. Choose action with $\\epsilon$-greedy**\n",
    "\n",
    "- Suppose $\\epsilon = 0.1$ → Random chance = 10%\n",
    "\n",
    "- Luckily pick Random → Use Q-network to predict:\n",
    "$$\n",
    "Q(s, a=0) = 0.4,\\quad Q(s, a=1) = 0.6\n",
    "$$\n",
    "\n",
    "- Choose **action = 1** (right) because Q is highest\n",
    "\n",
    "**2. Send action to environment**\n",
    "\n",
    "- Got:\n",
    "\n",
    "  - reward = 1\n",
    "\n",
    "  - next_state = [0.06, 0.025, -0.015, 0.035]\n",
    "\n",
    "  - done = False (Not yet failed)\n",
    "\n",
    "**3. Store transition**\n",
    "\n",
    "- Store $(s, a=1, r=1, s', done=False)$ in the replay buffer.\n",
    "\n",
    "**4. Assume that the buffer is sufficient → Start training 1 round**\n",
    "\n",
    "- Suppose that we store the transition in the replay buffer and randomly get 2 mini-batch samples as follows:\n",
    "\n",
    "| Index | State (s)                  | Action (a) | Reward (r) | Next State (s′)             | Done  |\n",
    "|-------|----------------------------|------------|------------|------------------------------|--------|\n",
    "| 0     | [0.05, 0.02, -0.01, 0.03]  | 1          | 1.0        | [0.06, 0.025, -0.015, 0.035] | False  |\n",
    "| 1     | [-0.01, -0.03, 0.02, -0.02] | 0         | 1.0        | [0.00, -0.02, 0.01, -0.01]   | True   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Calculate Q(s, a) from policy network**\n",
    "\n",
    "Suppose the policy network gives:\n",
    "\n",
    "| Index | Q(s, a=0) | Q(s, a=1) |\n",
    "|-------|-----------|-----------|\n",
    "| 0 | 0.5 | 0.65 |\n",
    "| 1 | 0.6 | 0.55 |\n",
    "\n",
    "Get Q of the selected action:\n",
    "\n",
    "- Example 0: action = 1 → Q = 0.65\n",
    "\n",
    "- Example 1: action = 0 → Q = 0.6\n",
    "\n",
    "**Q(s,a) = [0.65, 0.6]**\n",
    "\n",
    "**Calculate Target Q(s′, a′) from target network**\n",
    "\n",
    "Suppose Target Network gives:\n",
    "\n",
    "| Index | Q(s′, a=0) | Q(s′, a=1) | Done |\n",
    "|-------|------------|------------|--------|\n",
    "| 0 | 0.4 | 0.6 | False |\n",
    "| 1 | -- | -- | True |\n",
    "\n",
    "→ max Q(s′) only for not done:\n",
    "\n",
    "- Index 0: max = 0.6 \n",
    "\n",
    "- Index 1: terminal → max = 0\n",
    "\n",
    "**max_next_q_values ​​= [0.6, 0.0]**\n",
    "\n",
    "**7. Calculate Target Q-value (Bellman target)**\n",
    "\n",
    "Use the formula:\n",
    "$$\n",
    "y = r + \\gamma \\cdot (1 - \\text{done}) \\cdot \\max Q(s', a')\n",
    "$$\n",
    "\n",
    "ให้ $\\gamma = 0.99$:\n",
    "\n",
    "- Index 0:  $y = 1.0 + 0.99 \\cdot 0.6 = 1.594$\n",
    "\n",
    "- Index 1:  $y = 1.0 + 0 = 1.0$\n",
    "\n",
    "**Target Q-values = [1.594, 1.0]**\n",
    "\n",
    "**8. Calculate Loss**\n",
    "\n",
    "Use the formula:\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\sum_i (Q(s_i, a_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "- $Q(s_i, a_i)$ is Q-value from policy network\n",
    "\n",
    "- $y_i$ is Q-value from target network\n",
    "\n",
    "Substitute the value:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\left[ (0.65 - 1.594)^2 + (0.6 - 1.0)^2 \\right] \\\\\n",
    "= \\frac{1}{2} \\left[ 0.891 + 0.16 \\right] = \\frac{1.051}{2} = \\mathbf{0.5255}\n",
    "$$\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Index | Q(s, a) | Target y | Loss per item |\n",
    "|-------|---------|----------|----------------|\n",
    "| 0     | 0.65    | 1.594    | 0.891          |\n",
    "| 1     | 0.6     | 1.0      | 0.160          |\n",
    "|       |         |          | Total = 1.051 |\n",
    "|       |         |          | Avg = 0.5255  |\n",
    "\n",
    "**Final Loss = 0.5255**\n",
    "\n",
    "**9. Do Gradient Descent**\n",
    "\n",
    "- Call `loss.backward()` to compute gradient\n",
    "\n",
    "- Call `optimizer.step()` to update policy network weights\n",
    "\n",
    "**This is where Q-network learns from the latest experience that is randomly selected from the replay buffer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**MC-REINFORCE**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About MC-REINFORCE Algorithm**\n",
    "\n",
    "    - MC-REINFORCE (Monte Carlo REINFORCE) is the basic algorithm of Policy Gradient Method, using the concept of learning policy directly, not estimating Q-function like Q-learning.\n",
    "\n",
    "    - MC-REINFORCE is Policy-based because REINFORCE does not learn Q-value or V-value, but learns policy $\\pi_\\theta$ directly (pure policy gradient method).\n",
    "\n",
    "    - The type of policy is Stochastic policy because it uses $\\pi_\\theta(a|s)$ (e.g. softmax, categorical), not argmax, which is a deterministic policy. For example, $\\pi_\\theta(a=0|s) = 0.4$, $\\pi_\\theta(a=1|s) = 0.6$. We will sample actions according to these probabilities, not argmax, which will give us action 1.\n",
    "\n",
    "    - The Observation space is Continuous, while the Action space can be Discrete or Continuous, depending on the policy model chosen. If it is discrete, softmax must be used with linear output, but if it is continuous, Gaussian must be used with linear output instead.\n",
    "\n",
    "    - Exploration comes directly from stochastic policy because the agent has a chance to randomly take an action every time. If the policy learns that an action has a high reward, then the probability of that action will automatically increase, which will become exploitation.\n",
    "\n",
    "- **MC-REINFORCE ใช้สูตรของ Monte Carlo Policy Gradient**\n",
    "    $$\n",
    "    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t \\right]\n",
    "    $$\n",
    "\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\pi_\\theta(a_t | s_t)$ is the policy (e.g. softmax over linear output or neural net)\n",
    "\n",
    "    - $G_t$ is the total return from timestep $t$ to the end of the episode\n",
    "\n",
    "    - We keep the entire trajectory until the end (Monte Carlo) and then update\n",
    "\n",
    "Overall steps:\n",
    "\n",
    "- Randomize trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)$ by sampling from policy $\\pi_\\theta$\n",
    "\n",
    "- Compute return $G_t$ from timestep $t$ to end of episode\n",
    "\n",
    "- Compute gradient: $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "- Adjust policy weights with `gradient ascent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of MC-REINFORCE in CartPole**\n",
    "\n",
    "Use softmax policy network:$π_θ(a∣s)=softmax()$\n",
    "\n",
    "- Current state: $s_0 = [0.1, 0.0, 0.05, -0.02]$\n",
    "\n",
    "**1. Randomize trajectory from policy**\n",
    "\n",
    "Suppose agent randomizes this trajectory:\n",
    "\n",
    "| t | State $s_t$ | Action $a_t$ | Reward $r_{t+1}$ | $\\pi_\\theta(a_t \\mid s_t)$ |\n",
    "|---|-------------|---------------|-------------------|-----------------------------|\n",
    "| 0 | $s_0$       | 1             | 1                 | 0.6                         |\n",
    "| 1 | $s_1$       | 0             | 1                 | 0.4                         |\n",
    "| 2 | $s_2$       | 1             | 1                 | 0.7                         |\n",
    "| 3 | $s_3$       | 1             | 1                 | 0.8                         |\n",
    "\n",
    "Episode ends at timestep 4 → get reward = 1 at every timestep\n",
    "\n",
    "**2. Calculate Return $G_t$**\n",
    "\n",
    "Let $\\gamma = 0.99$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_3 &= r_4 = 1 \\\\\n",
    "G_2 &= r_3 + \\gamma G_3 = 1 + 0.99 \\cdot 1 = 1.99 \\\\\n",
    "G_1 &= r_2 + \\gamma G_2 = 1 + 0.99 \\cdot 1.99 = 2.9701 \\\\\n",
    "G_0 &= r_1 + \\gamma G_1 = 1 + 0.99 \\cdot 2.9701 = 3.9404\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "|Time t\t|$G_t$|\n",
    "|-------|---------|\n",
    "0|\t3.9404|\n",
    "1|\t2.9701|\n",
    "2|\t1.99|\n",
    "3|\t1.0|\n",
    "\n",
    "**3. Calculate the gradient from each step**\n",
    "\n",
    "We will use:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t\n",
    "$$\n",
    "\n",
    "- Step 0 example:\n",
    "\n",
    "    - Suppose the policy network predicts softmax:\n",
    "\n",
    "    $$\n",
    "    \\pi(a = 0 \\mid s_0) = 0.4,\\quad \\pi(a = 1 \\mid s_0) = 0.6\n",
    "    $$\n",
    "\n",
    "    - Choose $a_0 = 1$\n",
    "\n",
    "    - We get:\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta \\log \\pi_\\theta(1 \\mid s_0) = \\nabla_\\theta \\log(0.6)\n",
    "    $$\n",
    "\n",
    "    - Multiply by return:\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta J \\leftarrow \\nabla_\\theta \\log(0.6) \\cdot 3.9404\n",
    "    $$\n",
    "\n",
    "- Step 1:\n",
    "\n",
    "    - $\\log \\pi(0 \\mid s_1) = \\log(0.4) \\approx -0.9163$\n",
    "\n",
    "    - $-(-0.9163) \\cdot 2.9701 = 2.722$\n",
    "\n",
    "- Step 2:\n",
    "\n",
    "    - $\\log \\pi(1 \\mid s_2) = \\log(0.7) \\approx -0.3567$\n",
    "\n",
    "    - $-(-0.3567) \\cdot 1.9900 = 0.709$\n",
    "\n",
    "- Step 3:\n",
    "\n",
    "    - $\\log \\pi(1 \\mid s_3) = \\log(0.8) \\approx -0.2231$\n",
    "\n",
    "    - $-(-0.2231) \\cdot 1.0000 = 0.223$\n",
    "\n",
    "- Total Loss = 2.013 + 2.722 + 0.709 + 0.223 = 5.667\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| t   | $\\log \\pi(a_t \\mid s_t)$ | $G_t$   | $-\\log \\pi \\cdot G_t$ |\n",
    "|-----|---------------------------|---------|-------------------------|\n",
    "| 0   | -0.5108                   | 3.9404  | 2.013                   |\n",
    "| 1   | -0.9163                   | 2.9701  | 2.722                   |\n",
    "| 2   | -0.3567                   | 1.9900  | 0.709                   |\n",
    "| 3   | -0.2231                   | 1.0000  | 0.223                   |\n",
    "|     |                           |         | **Total: 5.667**        |\n",
    "\n",
    "**4. Do Gradient Ascent**\n",
    "\n",
    "- Call `loss.backward()` to compute gradient\n",
    "\n",
    "- Call `optimizer.step()` to update policy network weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Advantage Actor-Critic (A2C)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About Advantage Actor Critic**\n",
    "\n",
    "    - The name says it all: **Actor** → is the policy $\\pi_\\theta(a \\mid s)$ that chooses the action, and **Critic** → is the value function $V_w(s)$ that evaluates how good the state is, using what's called the **Advantage Function** to indicate how good the action is compared to the average: $A(s, a) = Q(s, a) - V(s) \\approx r + \\gamma V(s') - V(s)$.\n",
    "\n",
    "    - MC-REINFORCE is Actor-Critic-based because REINFORCE learn both value-function (critic) and policy (actor).\n",
    "\n",
    "    - The type of policy is Stochastic policy because it uses $\\pi_\\theta(a|s)$ (e.g. softmax, categorical), not argmax, which is a deterministic policy. For example, $\\pi_\\theta(a=0|s) = 0.4$, $\\pi_\\theta(a=1|s) = 0.6$. We will sample actions according to these probabilities, not argmax, which will give us action 1.\n",
    "\n",
    "    - The Observation space is Continuous, while the Action space can be Discrete or Continuous, depending on the policy model chosen. If it is discrete, softmax must be used with linear output, but if it is continuous, Gaussian must be used with linear output instead.\n",
    "\n",
    "    - Exploration comes directly from stochastic policy because the agent has a chance to randomly take an action every time. If the policy learns that an action has a high reward, then the probability of that action will automatically increase, which will become exploitation.\n",
    "\n",
    "- **A2C Brief Process**\n",
    "\n",
    "    - Actor uses policy network to randomize action: $a_t \\sim \\pi_\\theta(a_t \\mid s_t)$\n",
    "\n",
    "    - Critic uses value network to predict: $V(s_t)$\n",
    "\n",
    "    - Play in environment → get $r_t$, $s_{t+1}$ → calculate Advantage\n",
    "\n",
    "    - Update **Actor** with gradient: $\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot A(s_t, a_t)$\n",
    "\n",
    "    - Update **Critic** with MSE loss: $L_{\\text{critic}} = \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example of Advantage Actor Critic in CartPole**\n",
    "\n",
    "**0. Create trajectory**\n",
    "\n",
    "**Example Episode (T = 3)**\n",
    "\n",
    "| t | $s_t$                           | $a_t$ | $r_t$ | $s_{t+1}$                        |\n",
    "|---|---------------------------------|-------|-------|----------------------------------|\n",
    "| 0 | [0.05, 0.01, 0.03, 0.0]         | 1     | 1.0   | [0.06, 0.015, 0.035, 0.01]       |\n",
    "| 1 | [0.06, 0.015, 0.035, 0.01]      | 1     | 1.0   | [0.07, 0.02, 0.04, 0.015]        |\n",
    "| 2 | [0.07, 0.02, 0.04, 0.015]       | 0     | 1.0   | [0.08, 0.025, 0.05, 0.02]        |\n",
    "\n",
    "**1. Calculate advantage, critic loss, actor loss**\n",
    "\n",
    "**Step 0:**\n",
    "\n",
    "- receive action, value and reward\n",
    "\n",
    "    - $s_0 = [0.05, 0.01, 0.03, 0.0]$\n",
    "\n",
    "    - Actor random action: $\\pi(a=1 \\mid s_0) = 0.6, \\pi(a=0 \\mid s_0) = 0.4$\n",
    "\n",
    "    - Critic calculate value: $V(s_0) = 1.5, V(s_1) = 1.8$\n",
    "\n",
    "    - Receive reward: $r_1 = 1.0$\n",
    "\n",
    "- **Calculate Advantage:**\n",
    "\n",
    "$$\n",
    "A(s_0, a_0) = r_1 + \\gamma V(s_1) - V(s_0) = 1.0 + 0.99 \\cdot 1.8 - 1.5 = 1.282\n",
    "$$\n",
    "\n",
    "- **Calculate Actor Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}} = -\\log \\pi(a_0 \\mid s_0) \\cdot A(s_0, a_0) = -\\log(0.6) \\cdot 1.282 \\approx 0.511 \\cdot 1.282 = 0.655\n",
    "$$\n",
    "\n",
    "- **Calculate Critic Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}} = \\left( r_1 + \\gamma V(s_1) - V(s_0) \\right)^2 = (1.282)^2 \\approx 1.644\n",
    "$$\n",
    "\n",
    "**Step 1:**\n",
    "\n",
    "- $s_1 = [0.06, 0.015, 0.035, 0.01]$\n",
    "\n",
    "- $\\pi(a=1|s_1) = 0.7$, $V(s_1) = 1.8$, $V(s_2) = 1.5$\n",
    "\n",
    "- $r_2 = 1.0$\n",
    "\n",
    "$$\n",
    "A(s_1, a_1) = 1 + 0.99 \\cdot 1.5 - 1.8 = 0.685\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}} = -\\log(0.7) \\cdot 0.685 \\approx 0.357 \\cdot 0.685 = 0.244\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}} = (0.685)^2 = 0.469\n",
    "$$\n",
    "\n",
    "**Step 2:**\n",
    "\n",
    "- $V(s_2) = 1.5$, $V(s_3) = 0.0$ (assume episode ended)\n",
    "\n",
    "$$\n",
    "A(s_2, a_2) = 1 + 0 - 1.5 = -0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}} = -\\log(0.3) \\cdot (-0.5) \\approx -(-1.204) \\cdot (-0.5) = 0.602\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}} = (-0.5)^2 = 0.25\n",
    "$$\n",
    "\n",
    "**2. average Loss (Episode Summary)**\n",
    "\n",
    "| Step | Actor Loss | Critic Loss |\n",
    "|------|------------|-------------|\n",
    "| 0    | 0.655      | 1.644       |\n",
    "| 1    | 0.244      | 0.469       |\n",
    "| 2    | 0.602      | 0.250       |\n",
    "| total | **1.501** | **2.363**   |\n",
    "| avg | **0.5** | **0.788**   |\n",
    "\n",
    "**3. update Neural Net**\n",
    "\n",
    "```python\n",
    "avg_actor_loss.backward()\n",
    "avg_actor_loss.optimizer.step()\n",
    "\n",
    "avg_critic_loss.backward()\n",
    "avg_critic_loss.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 2: Setting up Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous homework, you will implement a common components that will be the same in most of the function approximation-based RL in the RL_base_function.py.The core components should include, but are not limited to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**1. RL Base class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - Number of actions: The total number of discrete actions available to the agent.\n",
    "\n",
    "        - Action range: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "        - Discretize state weight: Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "        - Learning rate: Determines how quickly the model updates based on new information.\n",
    "\n",
    "        - Initial epsilon: The starting probability of taking a random action in an ε-greedy policy.\n",
    "\n",
    "        - Epsilon decay rate: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "        - Final epsilon: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "        - Discount factor: A coefficient (γ) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "        - Buffer size: Maximum number of experiences the buffer can hold.\n",
    "\n",
    "        - Batch size: Number of experiences to sample per batch.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - scale_action(): scale the action (if it is computed from the sigmoid or softmax function) to the proper length.\n",
    "\n",
    "        - decay_epsilon(): Decreases epsilon over time and returns the updated value.\n",
    "\n",
    "- Additional details about these functions are provided in the class file. You may also implement additional functions for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**scale_action()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_action(self, action):\n",
    "    \"\"\"\n",
    "    Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "    Args:\n",
    "        action (int): Discrete action in range [0, n].\n",
    "        n (int): Number of discrete actions (inclusive range from 0 to n).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled action tensor.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Unpack the minimum and maximum values of the action range\n",
    "    action_min, action_max = self.action_range\n",
    "\n",
    "    # Scale the discrete action index (0 to num_of_action-1) to a continuous value within [action_min, action_max]\n",
    "    scaled = action_min + (action / (self.num_of_action - 1)) * (action_max - action_min)\n",
    "\n",
    "    # Check if the scaled value is already a torch.Tensor\n",
    "    if isinstance(scaled, torch.Tensor):\n",
    "        # If yes, detach it from any computation graph and convert to float32\n",
    "        return scaled.clone().detach().to(dtype=torch.float32)\n",
    "    else:\n",
    "        # Otherwise, convert it into a torch.Tensor of type float32\n",
    "        return torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**decay_epsilon()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self):\n",
    "    \"\"\"\n",
    "    Decay epsilon value to reduce exploration over time.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Decay the exploration rate (epsilon) by multiplying with epsilon_decay,\n",
    "    # but ensure it doesn't go below the minimum value (final_epsilon)\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**q()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(self, obs, a=None):\n",
    "    # Ensure obs has batch dimension\n",
    "    if obs.dim() == 1:\n",
    "        obs = obs.view(1, -1)  # make it [1, obs_dim]\n",
    "\n",
    "    # Compute linear Q-values: Q(s) = obs @ w\n",
    "    q_values = obs @ self.w  \n",
    "\n",
    "    if a is None:\n",
    "        return q_values.squeeze(0) # Return Q-values for all actions\n",
    "    else:\n",
    "        return q_values[0, a] # Return Q-value for specific action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**2. Replay Buffer Class**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- A class use to store state, action, reward, next state, and termination status from each timestep in episode to use as a dataset to train neural networks. This class should include:\n",
    "\n",
    "    - Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "        - memory: FIFO buffer to store the trajectory within a certain time window.\n",
    "\n",
    "        - batch_size: Number of data samples drawn from memory to train the neural network.\n",
    "\n",
    "    - Core Functions\n",
    "\n",
    "        - add(): Add state, action, reward, next state, and termination status to the FIFO buffer. Discard the oldest data in the buffer\n",
    "\n",
    "        - sample(): Sample data from memory to use in the neural network training.\n",
    "\n",
    "    - <font color=\"orange\">**Note that some algorithms may not use all of the data mentioned above to train the neural network.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**add()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Adds an experience to the replay buffer.\n",
    "\n",
    "    Args:\n",
    "        state (Tensor): The current state of the environment.\n",
    "        action (Tensor): The action taken at this state.\n",
    "        reward (Tensor): The reward received after taking the action.\n",
    "        next_state (Tensor): The next state resulting from the action.\n",
    "        done (bool): Whether the episode has terminated.\n",
    "    \"\"\"\n",
    "\n",
    "    self.memory.append(Transition(state, action, next_state, reward, done)) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**sample()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\"\n",
    "    )\n",
    "\n",
    "def sample(self):\n",
    "    \"\"\"\n",
    "    Samples a batch of experiences from the replay buffer.\n",
    "\n",
    "    Returns:\n",
    "        - state_batch: Batch of states.\n",
    "        - action_batch: Batch of actions.\n",
    "        - reward_batch: Batch of rewards.\n",
    "        - next_state_batch: Batch of next states.\n",
    "        - done_batch: Batch of terminal state flags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Random Batch\n",
    "    transitions = random.sample(self.memory, self.batch_size)\n",
    "    batch = Transition(*zip(*transitions)) # type: ignore\n",
    "\n",
    "    # Transform to tensor\n",
    "    state_batch = torch.stack(batch.state).to(device)\n",
    "    action_batch = torch.stack(batch.action).to(device)\n",
    "    next_state_batch = torch.stack(batch.next_state).to(device)        \n",
    "    reward_batch = torch.stack(batch.reward).to(device)\n",
    "    done_batch = torch.tensor(batch.done, dtype=torch.bool).to(device)\n",
    "\n",
    "    return state_batch, action_batch, reward_batch, next_state_batch, done_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">**3. Algorithm folder**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This folder should include:\n",
    "\n",
    "    - Linear Q Learning class\n",
    "\n",
    "    - Deep Q-Network class\n",
    "\n",
    "    - REINFORCE Class\n",
    "\n",
    "    - One class chosen from the Part 1.\n",
    "\n",
    "- Each class should inherit from the RL Base class in RL_base_function.py and include:\n",
    "\n",
    "    - A constructor which initializes the same variables as the class it inherits from.\n",
    "\n",
    "    - Superclass Initialization (super().__init__()).\n",
    "\n",
    "    - An update() function that updates the agent’s learnable parameters and advances the training step.\n",
    "\n",
    "    - A select_action() function select the action according to current policy.\n",
    "\n",
    "    - A learn() function that train the regression or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Linear Q-Learning class**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        device: None,\n",
    "        num_of_action: int = 2,\n",
    "        n_observations: int = 4,\n",
    "        action_range: list = [-2.5, 2.5],\n",
    "        learning_rate: float = 0.01,\n",
    "        initial_epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 1e-3,\n",
    "        final_epsilon: float = 0.001,\n",
    "        discount_factor: float = 0.95,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Initialize the CartPole Agent.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate for updating Q-values.\n",
    "        initial_epsilon (float): The initial exploration rate.\n",
    "        epsilon_decay (float): The rate at which epsilon decays over time.\n",
    "        final_epsilon (float): The final exploration rate.\n",
    "        discount_factor (float, optional): The discount factor for future rewards. Defaults to 0.95.\n",
    "    \"\"\"        \n",
    "\n",
    "    super().__init__(\n",
    "        num_of_action=num_of_action,\n",
    "        action_range=action_range,\n",
    "        learning_rate=learning_rate,\n",
    "        initial_epsilon=initial_epsilon,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "        final_epsilon=final_epsilon,\n",
    "        discount_factor=discount_factor,\n",
    "    )\n",
    "\n",
    "    self.device = device\n",
    "    self.episode_durations = []\n",
    "\n",
    "    # Initialize the weight matrix for linear Q-value approximation.\n",
    "    # Shape: [state_dim, num_actions]\n",
    "    self.w = torch.zeros((n_observations, num_of_action), dtype=torch.float32, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(\n",
    "    self,\n",
    "    obs,\n",
    "    action,\n",
    "    reward,\n",
    "    next_obs,\n",
    "    terminated\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the weight vector using the Temporal Difference (TD) error \n",
    "    in Q-learning with linear function approximation.\n",
    "\n",
    "    Args:\n",
    "        obs (dict): The current state observation, containing feature representations.\n",
    "        action (int): The action taken in the current state.\n",
    "        reward (float): The reward received for taking the action.\n",
    "        next_obs (dict): The next state observation.\n",
    "        next_action (int): The action taken in the next state (used in SARSA).\n",
    "        terminated (bool): Whether the episode has ended.\n",
    "\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    \"\"\"\n",
    "    Q-learning with linear function approximation.\n",
    "    \"\"\"\n",
    "    # Convert observations to tensor and move to appropriate device\n",
    "    phi_s = obs.clone().detach().to(self.device)  # current features\n",
    "    phi_next = next_obs.clone().detach().to(self.device) if not terminated else torch.zeros_like(phi_s)\n",
    "\n",
    "    # Estimate current and target Q-values\n",
    "    current_q = self.q(phi_s, a=action)\n",
    "    next_q = torch.max(self.q(phi_next)).item() if not terminated else 0.0\n",
    "\n",
    "    # Calculate TD target and TD error\n",
    "    td_target = reward + self.discount_factor * next_q\n",
    "    td_error = td_target - current_q.item()\n",
    "\n",
    "    # Apply gradient update to the weight vector for the taken action\n",
    "    self.w[:, action] += self.lr * td_error * phi_s\n",
    "\n",
    "    # Log TD error for analysis\n",
    "    self.training_error.append(td_error)\n",
    "\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    \"\"\"\n",
    "    Select an action based on an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        state (Tensor): The current state of the environment.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The selected action.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    state_tensor = state.to(self.device)\n",
    "\n",
    "    # Explore: with probability epsilon, select a random action\n",
    "    if torch.rand(1).item() < self.epsilon:\n",
    "        return torch.randint(0, self.num_of_action, (1,)).item()\n",
    "    else:\n",
    "        # Exploit: choose the action with the highest Q-value\n",
    "        q_values = self.q(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env, max_steps):\n",
    "    \"\"\"\n",
    "    Train the agent on a single step.\n",
    "\n",
    "    Args:\n",
    "        env: The environment in which the agent interacts.\n",
    "        max_steps (int): Maximum number of steps per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== Initialize trajectory collection variables ===== #\n",
    "    # Reset environment to get initial state (tensor)\n",
    "    # Track total episode return (float)\n",
    "    # Flag to indicate episode termination (boolean)\n",
    "    # Step counter (int)\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Reset environment and initialize counters\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    # Run until episode ends or max_steps is reached\n",
    "    while not done or steps < max_steps:\n",
    "\n",
    "        # Extract state vector (shape: [4]) from observation dictionary\n",
    "        obs_list = torch.tensor([obs['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Choose action based on current policy\n",
    "        action = self.select_action(obs_list)\n",
    "\n",
    "        # Scale the discrete action to match environment's action space\n",
    "        scaled_action = self.scale_action(action).view(1, -1)\n",
    "\n",
    "        # Take an action in the environment\n",
    "        next_obs, reward, terminated, truncated, _  = env.step(scaled_action)\n",
    "\n",
    "        # Extract next state vector\n",
    "        next_obs_list = torch.tensor([next_obs['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Determine whether the episode is over\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update weight with new experience\n",
    "        self.update(obs_list, action, reward, next_obs_list, done)\n",
    "\n",
    "        # Accumulate reward and move to next state\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        # If episode ends, decay epsilon and return stats\n",
    "        if done:\n",
    "            self.decay_epsilon()\n",
    "            return steps, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**Deep Q-Network (DQN) class**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_network(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for the Deep Q-Network algorithm.\n",
    "    \n",
    "    Args:\n",
    "        n_observations (int): Number of input features.\n",
    "        hidden_size (int): Number of hidden neurons.\n",
    "        n_actions (int): Number of possible actions.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(DQN_network, self).__init__()\n",
    "\n",
    "        # === Define layers (2 hidden layer) ===\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size)  # Input → Hidden\n",
    "        self.dropout = nn.Dropout(dropout)                 # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)     # Hidden → Hidden\n",
    "        self.out = nn.Linear(hidden_size, n_actions)       # Hidden → Output (Q-values)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights using Xavier initialization for better convergence.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input state tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Q-value estimates for each action.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x) # Output Q(s, a) for all actions\n",
    "        # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        device = None,\n",
    "        num_of_action: int = 11,\n",
    "        action_range: list = [-12.0, 12.0],\n",
    "        n_observations: int = 4,\n",
    "        hidden_dim: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "        learning_rate: float = 0.001,\n",
    "        tau: float = 0.005,\n",
    "        initial_epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 1e-3,\n",
    "        final_epsilon: float = 0.05,\n",
    "        discount_factor: float = 0.95,\n",
    "        buffer_size: int = 10000,\n",
    "        batch_size: int = 32\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Initialize the CartPole Agent.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate for updating Q-values.\n",
    "        initial_epsilon (float): The initial exploration rate.\n",
    "        epsilon_decay (float): The rate at which epsilon decays over time.\n",
    "        final_epsilon (float): The final exploration rate.\n",
    "        discount_factor (float, optional): The discount factor for future rewards. Defaults to 0.95.\n",
    "    \"\"\"     \n",
    "    \n",
    "\n",
    "\n",
    "    # === Set up networks ===\n",
    "    self.policy_net = DQN_network(n_observations, self.hidden_dim, self.num_of_action, dropout).to(device)\n",
    "    self.target_net = DQN_network(n_observations, self.hidden_dim, self.num_of_action, dropout).to(device)\n",
    "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    # === Set device and hyperparameters ===\n",
    "    self.device = device\n",
    "    self.tau = tau\n",
    "    self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.learning_rate, amsgrad=True)\n",
    "    self.episode_durations = []\n",
    "    self.buffer_size = buffer_size\n",
    "    self.batch_size = batch_size\n",
    "    self.initial_epsilon = initial_epsilon\n",
    "    self.state_stats = deque(maxlen=1000)\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.learning_rate = learning_rate\n",
    "    self.num_of_action = num_of_action        \n",
    "\n",
    "    super(DQN, self).__init__(\n",
    "        num_of_action=num_of_action,\n",
    "        action_range=action_range,\n",
    "        learning_rate=learning_rate,\n",
    "        initial_epsilon=initial_epsilon,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "        final_epsilon=final_epsilon,  \n",
    "        discount_factor=discount_factor,\n",
    "        buffer_size=buffer_size,\n",
    "        batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    \"\"\"\n",
    "    Select an action based on an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        state (Tensor): The current state of the environment.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The selected action.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    if torch.rand(1).item() < self.epsilon:\n",
    "        # random action index [0, num_of_action-1] shape: [1, 1]\n",
    "        return torch.tensor([[random.randrange(self.num_of_action)]], device=self.device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)  # shape: [1, num_actions]\n",
    "            action_idx = torch.argmax(q_values, dim=0, keepdim=True)  # shape: [1, 1]\n",
    "        return action_idx\n",
    "\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(self, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch sample from memory for training.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "            - non_final_mask (Tensor): A boolean mask indicating which states are non-final.\n",
    "            - non_final_next_states (Tensor): The next states that are not terminal.\n",
    "            - state_batch (Tensor): The batch of current states.\n",
    "            - action_batch (Tensor): The batch of actions taken.\n",
    "            - reward_batch (Tensor): The batch of rewards received.\n",
    "    \"\"\"\n",
    "    # Ensure there are enough samples in memory before proceeding\n",
    "\n",
    "    # Sample a batch from memory\n",
    "    # ========= put your code here ========= #\n",
    "    state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample()\n",
    "    if len(self.memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # Create mask True where next_state is NOT terminalfor non-final (non-terminal) states\n",
    "    non_final_mask = ~done_batch  # done = False → non_final\n",
    "    non_final_next_states = next_state_batch[non_final_mask]\n",
    "\n",
    "    return non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self,\n",
    "                    non_final_mask, \n",
    "                    non_final_next_states, \n",
    "                    state_batch, \n",
    "                    action_batch, \n",
    "                    reward_batch):\n",
    "    \"\"\"\n",
    "    Computes the loss for policy optimization.\n",
    "\n",
    "    Args:\n",
    "        non_final_mask (Tensor): Mask indicating which states are non-final.\n",
    "        non_final_next_states (Tensor): The next states that are not terminal.\n",
    "        state_batch (Tensor): Batch of current states.\n",
    "        action_batch (Tensor): Batch of actions taken.\n",
    "        reward_batch (Tensor): Batch of received rewards.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Computed loss.\n",
    "\n",
    "    This function:\n",
    "    - Predicts Q(s, a) using the policy network.\n",
    "    - Computes max Q(s', a') from the target network for non-terminal next states.\n",
    "    - Calculates the target Q-values using the Bellman equation: \n",
    "        target = r + y * max_a' Q_target(s', a')\n",
    "    - Computes the mean squared error (MSE) loss between predicted and target Q-values.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Reshape action_batch to match gather's requirements and ensure long type for indexing\n",
    "    action_batch = action_batch.view(1, -1).long()\n",
    "\n",
    "    dones = non_final_mask.float()\n",
    "\n",
    "    # Compute Q(s, a) from the policy network by selecting the Q-values for the taken actions\n",
    "    q_values = self.policy_net(state_batch).gather(1, action_batch).view(-1)\n",
    "\n",
    "    # Initialize a tensor to hold Q(s', a') values for each sample in the batch\n",
    "    next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "\n",
    "    # Compute Q(s', a') using the target network, but only for non-final next states\n",
    "    with torch.no_grad():\n",
    "        # Get Q-values from the target network for non-final next states\n",
    "        next_q_values = self.target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "        # Compute the target Q-values using the Bellman equation:\n",
    "        # target = reward + gamma * max_a' Q(s', a')\n",
    "        expected_q_values = reward_batch.squeeze(-1) + (1 - dones) * (self.discount_factor * next_q_values)\n",
    "\n",
    "    # Compute the mean squared error loss between predicted Q(s, a) and target Q-values\n",
    "    loss = F.mse_loss(q_values, expected_q_values)\n",
    "\n",
    "    # Return the computed loss for backpropagation\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self):\n",
    "    \"\"\"\n",
    "    Update the policy using the calculated loss.\n",
    "\n",
    "    Returns:\n",
    "        float: Loss value after the update.\n",
    "    \"\"\"\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "    # Generate a sample batch\n",
    "    sample = self.generate_sample(self.batch_size)\n",
    "    if sample is None:\n",
    "        return\n",
    "    non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch = sample\n",
    "\n",
    "    # Compute loss\n",
    "    loss = self.calculate_loss(non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch)\n",
    "    l2_reg = torch.norm(self.policy_net.fc1.weight, p=2) + torch.norm(self.policy_net.fc2.weight, p=2)\n",
    "    loss += 1e-4 * l2_reg\n",
    "\n",
    "    # Perform gradient descent step\n",
    "    # ========= put your code here ========= #\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_networks(self):\n",
    "    \"\"\"\n",
    "    Soft update of target network weights using Polyak averaging.\n",
    "    \"\"\"\n",
    "    # Retrieve the state dictionaries (weights) of both networks\n",
    "    # ========= put your code here ========= #\n",
    "    policy_state_dict = self.policy_net.state_dict()\n",
    "    target_state_dict = self.target_net.state_dict()\n",
    "    # ====================================== #\n",
    "    \n",
    "    # Apply the soft update rule to each parameter in the target network\n",
    "    # ========= put your code here ========= #\n",
    "    for key in target_state_dict:\n",
    "        target_state_dict[key] = (\n",
    "            self.tau * policy_state_dict[key] +\n",
    "            (1.0 - self.tau) * target_state_dict[key]\n",
    "        )\n",
    "    # ====================================== #\n",
    "    \n",
    "    # Load the updated weights into the target network\n",
    "    # ========= put your code here ========= #\n",
    "    self.target_net.load_state_dict(target_state_dict)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env, max_steps):\n",
    "    \"\"\"\n",
    "    Train the agent on a single step.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to train in.\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== Initialize trajectory collection variables ===== #\n",
    "    # Reset environment to get initial state (tensor)\n",
    "    # Track total episode return (float)\n",
    "    # Flag to indicate episode termination (boolean)\n",
    "    # Step counter (int)\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # === Episode Initialization ===\n",
    "    # - Reset the environment and extract the initial observation\n",
    "    # - Convert observation to tensor and set initial state\n",
    "    # - Initialize reward tracker, timestep counter, and done flag\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "    timestep = 0\n",
    "    total_loss = 0.0\n",
    "    # ====================================== #\n",
    "\n",
    "    while not done or timestep < max_steps:\n",
    "        obs_list = torch.tensor([obs['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "        state = torch.tensor(obs_list, dtype=torch.float32, device=self.device).unsqueeze(0) \n",
    "        # === Action Selection ===\n",
    "        # - Select action index using epsilon-greedy strategy\n",
    "        # - Convert index to actual action value if necessary\n",
    "        action_idx = self.select_action(obs_list)\n",
    "        action = self.scale_action(action_idx).view(1, -1)\n",
    "\n",
    "        # === Environment Interaction ===\n",
    "        # - Take one step in the environment\n",
    "        # - Extract next state, reward, and done flags\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_obs_list = torch.tensor([next_obs['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = torch.tensor(next_obs_list, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        reward_tensor = torch.as_tensor(reward, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # === Store Transition ===\n",
    "        # - Ensure state/action tensor shapes are correct\n",
    "        # - Store (s, a, r, s', done) in the replay buffer\n",
    "        self.memory.add(state, action_idx, reward_tensor, next_state, done)\n",
    "\n",
    "        # === Learning and Target Update ===\n",
    "        # - Update Q-network if buffer has enough samples\n",
    "        # - Soft update target network every fixed interval\n",
    "        \n",
    "        if len(self.memory) > 1000: # Perform one step of the optimization (on the policy network)\n",
    "            loss = self.update_policy()\n",
    "        \n",
    "        # Soft update of the target network's weights\n",
    "        self.update_target_networks()\n",
    "\n",
    "        # === Bookkeeping ===\n",
    "        # - Track total reward, step counters, and check termination\n",
    "        total_reward += reward\n",
    "        timestep += 1\n",
    "        obs = next_obs\n",
    "\n",
    "        # === End-of-Episode Handling ===\n",
    "        # - Decay epsilon for exploration\n",
    "        # - Plot training progress\n",
    "        if done:\n",
    "            self.decay_epsilon()\n",
    "            return timestep, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save weight parameters.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    file_path = os.path.join(path, filename)\n",
    "    torch.save(self.policy_net.state_dict(), file_path)\n",
    "    print(f\"[INFO] Saved model weights to {file_path}\")\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load weight parameters.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    file_path = os.path.join(path, filename)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        self.policy_net.load_state_dict(torch.load(file_path, map_location=self.device))\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())  # Sync target with policy\n",
    "        print(f\"[INFO] Loaded model weights from {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"[ERROR] File not found: {file_path}\")\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**REINFORCE class**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_REINFORCE_network(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for the MC_REINFORCE algorithm.\n",
    "    \n",
    "    Args:\n",
    "        n_observations (int): Number of input features.\n",
    "        hidden_size (int): Number of hidden neurons.\n",
    "        n_actions (int): Number of possible actions.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(MC_REINFORCE_network, self).__init__()\n",
    "        # ========= put your code here ========= #\n",
    "\n",
    "        # one layer neural network\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, n_actions)\n",
    "        # ====================================== #\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor representing action probabilities.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        action_probs = F.softmax(x, dim=-1) # softmax for discrete action\n",
    "        action_probs = torch.clamp(action_probs, min=1e-8, max=1.0)\n",
    "        return action_probs\n",
    "        # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        device = None,\n",
    "        num_of_action: int = 2,\n",
    "        action_range: list = [-2.5, 2.5],\n",
    "        n_observations: int = 4,\n",
    "        hidden_dim: int = 64,\n",
    "        dropout: float = 0.5,\n",
    "        learning_rate: float = 0.01,\n",
    "        discount_factor: float = 0.95,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Initialize the CartPole Agent.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate for updating Q-values.\n",
    "        initial_epsilon (float): The initial exploration rate.\n",
    "        epsilon_decay (float): The rate at which epsilon decays over time.\n",
    "        final_epsilon (float): The final exploration rate.\n",
    "        discount_factor (float, optional): The discount factor for future rewards. Defaults to 0.95.\n",
    "    \"\"\"     \n",
    "\n",
    "    # ========= put your code here ========= #\n",
    "    self.LR = learning_rate\n",
    "\n",
    "    self.policy_net = MC_REINFORCE_network(n_observations, hidden_dim, num_of_action, dropout).to(device)\n",
    "    self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    self.device = device\n",
    "    self.hidden = hidden_dim\n",
    "    self.steps_done = 0\n",
    "\n",
    "    self.episode_durations = []\n",
    "    \n",
    "    # ====================================== #\n",
    "\n",
    "    super(MC_REINFORCE, self).__init__( # type: ignore\n",
    "        num_of_action=num_of_action,\n",
    "        action_range=action_range,\n",
    "        learning_rate=learning_rate,\n",
    "        discount_factor=discount_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stepwise_returns(self, rewards):\n",
    "    \"\"\"\n",
    "    Compute stepwise returns for the trajectory.\n",
    "\n",
    "    Args:\n",
    "        rewards (list): List of rewards obtained in the episode.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Normalized stepwise returns.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Monte Carlo Return Calculation\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + self.discount_factor * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "    returns = F.normalize(returns, dim=0)\n",
    "\n",
    "    return returns\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(self, env):\n",
    "    \"\"\"\n",
    "    Generate a trajectory by interacting with the environment.\n",
    "\n",
    "    Args:\n",
    "        env: The environment object.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (episode_return, stepwise_returns, log_prob_actions, trajectory)\n",
    "    \"\"\"\n",
    "    # ===== Initialize trajectory collection variables ===== #\n",
    "    # Reset environment to get initial state (tensor)\n",
    "    # Store state-action-reward history (list)\n",
    "    # Store log probabilities of actions (list)\n",
    "    # Store rewards at each step (list)\n",
    "    # Track total episode return (float)\n",
    "    # Flag to indicate episode termination (boolean)\n",
    "    # Step counter (int)\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Initialization\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    trajectory = []\n",
    "    log_prob_actions = []\n",
    "    rewards = []\n",
    "    entropy_list = []\n",
    "\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    episode_return = 0.0\n",
    "    # ====================================== #\n",
    "    \n",
    "    # Trajectory Collection Loop\n",
    "    while not done:\n",
    "        \n",
    "        # Get state tensor and predict action\n",
    "        state_tensor = torch.tensor([state['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "        action_probs = self.policy_net(state_tensor)\n",
    "        dist = distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Get log-probability and entropy of action\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        entropy_list.append(entropy)\n",
    "\n",
    "        # Scale action and step environment\n",
    "        action = action.view(1, -1)\n",
    "        action = self.scale_action(action)            \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Save log-prob, reward, and full transition\n",
    "        log_prob_actions.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        trajectory.append((state_tensor, action.item(), reward))\n",
    "\n",
    "        # Update state and counters\n",
    "        state = next_state\n",
    "        episode_return += reward\n",
    "        timestep += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Prepare return values\n",
    "    stepwise_returns = self.calculate_stepwise_returns(rewards)\n",
    "    log_prob_actions = torch.stack(log_prob_actions)\n",
    "    return timestep, episode_return, stepwise_returns, log_prob_actions, trajectory, torch.stack(entropy_list)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, stepwise_returns, log_prob_actions, entropy_list):\n",
    "    \"\"\"\n",
    "    Compute the loss for policy optimization.\n",
    "\n",
    "    Args:\n",
    "        stepwise_returns (Tensor): Stepwise returns for the trajectory.\n",
    "        log_prob_actions (Tensor): Log probabilities of actions taken.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Computed loss.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Policy Gradient Loss\n",
    "    return -torch.sum(log_prob_actions * stepwise_returns)/len(stepwise_returns)\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self, stepwise_returns, log_prob_actions, entropy_list):\n",
    "    \"\"\"\n",
    "    Update the policy using the calculated loss.\n",
    "\n",
    "    Args:\n",
    "        stepwise_returns (Tensor): Stepwise returns.\n",
    "        log_prob_actions (Tensor): Log probabilities of actions taken.\n",
    "    \n",
    "    Returns:\n",
    "        float: Loss value after the update.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Calculate loss\n",
    "    loss = self.calculate_loss(stepwise_returns, log_prob_actions, entropy_list)\n",
    "\n",
    "    # Backward pass and optimizer step\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env):\n",
    "    \"\"\"\n",
    "    Train the agent on a single episode.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to train in.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (episode_return, loss, trajectory)\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Train policy for 1 episode\n",
    "    self.policy_net.train()\n",
    "    timestep, episode_return, stepwise_returns, log_prob_actions, trajectory, entropy = self.generate_trajectory(env)\n",
    "    \n",
    "    # Policy update\n",
    "    loss = self.update_policy(stepwise_returns, log_prob_actions, entropy)\n",
    "    \n",
    "    # Return training info\n",
    "    return timestep, episode_return, stepwise_returns, loss, trajectory\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save model weight.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    file_path = os.path.join(path, filename)\n",
    "    torch.save(self.policy_net.state_dict(), file_path)\n",
    "    print(f\"[INFO] Saved model weights to {file_path}\")\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load model weight.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    file_path = os.path.join(path, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        self.policy_net.load_state_dict(torch.load(file_path, map_location=self.device))\n",
    "        print(f\"[INFO] Loaded model weights from {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"[ERROR] File not found: {file_path}\")\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">**A2C class**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Actor network for policy approximation.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the state space.\n",
    "            hidden_dim (int): Number of hidden units in layers.\n",
    "            output_dim (int): Dimension of the action space.\n",
    "            learning_rate (float, optional): Learning rate for optimization. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        # ========= put your code here ========= #\n",
    "\n",
    "        # Define the neural network structure (one layer neural network for predict output action)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Set up the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Initialize network weights\n",
    "        self.init_weights()\n",
    "        # ====================================== #\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights using Xavier initialization for better convergence.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass for action selection.\n",
    "\n",
    "        Args:\n",
    "            state (Tensor): Current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Selected action values.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        x = self.net(state)\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "        return x\n",
    "        # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Critic network for Q-value approximation.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space.\n",
    "            action_dim (int): Dimension of the action space.\n",
    "            hidden_dim (int): Number of hidden units in layers.\n",
    "            learning_rate (float, optional): Learning rate for optimization. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # ========= put your code here ========= #\n",
    "\n",
    "        # Define the neural network (one layer neural network) for value prediction\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1) # Output is scalar V(s)\n",
    "        )\n",
    "\n",
    "        # Set up the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Initialize weights \n",
    "        self.init_weights()\n",
    "        # ====================================== #\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights using Kaiming initialization.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  # Kaiming initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Q-value estimation.\n",
    "\n",
    "        Args:\n",
    "            state (Tensor): Current state of the environment.\n",
    "            action (Tensor): Action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Estimated Q-value.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        return self.net(state).squeeze(-1)  # output shape: [batch]\n",
    "        # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, \n",
    "            device = None, \n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim = 256,\n",
    "            dropout = 0.05, \n",
    "            learning_rate: float = 0.01,\n",
    "            tau: float = 0.005,\n",
    "            discount_factor: float = 0.95,\n",
    "            buffer_size: int = 256,\n",
    "            batch_size: int = 1,\n",
    "            entropy_coeff: float = 0.01,\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Actor-Critic algorithm implementation.\n",
    "\n",
    "    Args:\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "        num_of_action (int, optional): Number of possible actions. Defaults to 2.\n",
    "        action_range (list, optional): Range of action values. Defaults to [-2.5, 2.5].\n",
    "        n_observations (int, optional): Number of observations in state. Defaults to 4.\n",
    "        hidden_dim (int, optional): Hidden layer dimension. Defaults to 256.\n",
    "        learning_rate (float, optional): Learning rate. Defaults to 0.01.\n",
    "        tau (float, optional): Soft update parameter. Defaults to 0.005.\n",
    "        discount_factor (float, optional): Discount factor for Q-learning. Defaults to 0.95.\n",
    "        batch_size (int, optional): Size of training batches. Defaults to 1.\n",
    "        buffer_size (int, optional): Replay buffer size. Defaults to 256.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set device and instantiate networks\n",
    "    self.device = device\n",
    "    self.actor = Actor(n_observations, hidden_dim, num_of_action, learning_rate).to(device)\n",
    "    self.critic = Critic(n_observations, num_of_action, hidden_dim, learning_rate).to(device)\n",
    "\n",
    "    # Store hyperparameters\n",
    "    self.discount_factor = discount_factor\n",
    "    self.action_range = action_range\n",
    "    self.entropy_coeff = entropy_coeff\n",
    "    self.learning_rate = learning_rate\n",
    "    self.hidden = hidden_dim\n",
    "\n",
    "    # Call parent class initializer\n",
    "    super(A2C_Discrete, self).__init__(\n",
    "        num_of_action=num_of_action,\n",
    "        action_range=action_range,\n",
    "        learning_rate=learning_rate,\n",
    "        discount_factor=discount_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    \"\"\"\n",
    "    Selects an action based on the current policy with optional exploration noise.\n",
    "    \n",
    "    Args:\n",
    "    state (Tensor): The current state of the environment.\n",
    "    noise (float, optional): The standard deviation of noise for exploration. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: \n",
    "            - scaled_action: The final action after scaling.\n",
    "            - clipped_action: The action before scaling but after noise adjustment.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    probs = self.actor(state)                       # Get action probabilities\n",
    "    probs = torch.clamp(probs, min=1e-6, max=1.0)   # Prevent extremely small probabilities\n",
    "    dist = torch.distributions.Categorical(probs)   # Create Categorical distribution\n",
    "    action = dist.sample()                          # Sample action from distribution\n",
    "    log_prob = dist.log_prob(action).sum(dim=-1)    # Compute log-probability of chosen action\n",
    "    return action, log_prob\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Computes the loss for policy optimization.\n",
    "\n",
    "    Args:\n",
    "        - states (Tensor): The batch of current states.\n",
    "        - actions (Tensor): The batch of actions taken.\n",
    "        - rewards (Tensor): The batch of rewards received.\n",
    "        - next_states (Tensor): The batch of next states received.\n",
    "        - dones (Tensor): The batch of dones received.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Computed critic & actor loss.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    # Convert to tensors\n",
    "    values = self.critic(state)             # V(s)\n",
    "    next_values = self.critic(next_state)   # V(s')\n",
    "\n",
    "    # Compute temporal difference target\n",
    "    td_target = reward + self.discount_factor * next_values\n",
    "\n",
    "    # Advantage is the difference between TD target and current value\n",
    "    advantage = td_target - values\n",
    "\n",
    "    # Actor Loss: negative log-prob * advantage\n",
    "    logit = self.actor(state)\n",
    "    dist = torch.distributions.Categorical(logit)\n",
    "    log_probs = dist.log_prob(action)\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "\n",
    "    # Critic Loss: MSE of advantage\n",
    "    critic_loss = (advantage**2).mean()\n",
    "\n",
    "    return actor_loss, critic_loss\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self, state, action, reward, next_state, done): \n",
    "    \"\"\"\n",
    "    Update the policy using the calculated loss.\n",
    "\n",
    "    Returns:\n",
    "        float: Loss value after the update.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Compute critic and actor loss\n",
    "    actor_loss, critic_loss = self.calculate_loss(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Backpropagate and update critic network parameters\n",
    "    self.actor.optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor.optimizer.step()\n",
    "\n",
    "    # Backpropagate and update actor network parameters\n",
    "    self.critic.optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic.optimizer.step()\n",
    "    # ====================================== #\n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env, max_steps, num_agents):\n",
    "    \"\"\"\n",
    "    Train the agent on a single step.\n",
    "\n",
    "    Args:\n",
    "        env: The environment in which the agent interacts.\n",
    "        max_steps (int): Maximum number of steps per episode.\n",
    "        num_agents (int): Number of agents in the environment.\n",
    "        noise_scale (float, optional): Initial exploration noise level. Defaults to 0.1.\n",
    "        noise_decay (float, optional): Factor by which noise decreases per step. Defaults to 0.99.\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== Initialize trajectory collection variables ===== #\n",
    "    # Reset environment to get initial state (tensor)\n",
    "    # Track total episode return (float)\n",
    "    # Flag to indicate episode termination (boolean)\n",
    "    # Step counter (int)\n",
    "    # ========= put your code here ========= #\n",
    "\n",
    "    # Reset environment\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "    num_step = 0\n",
    "    total_actorloss = 0.0\n",
    "    total_criticloss = 0.0\n",
    "    # ====================================== #\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "\n",
    "        # Convert observation to tensor\n",
    "        state_tensor = torch.tensor([state['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Select action from actor\n",
    "        action, log_prob = self.select_action(state_tensor)\n",
    "\n",
    "        # Scale and apply action in the environment\n",
    "        scaled_action = self.scale_action(action.item()).view(1, -1)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(scaled_action)\n",
    "        \n",
    "        # Process next state\n",
    "        done = terminated or truncated\n",
    "        next_state_tensor = torch.tensor([next_state['policy'][0, i] for i in range(4)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Update networks using this transition                        \n",
    "        actor_loss, critic_loss = self.update_policy(state_tensor, action, reward, next_state_tensor, done)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        total_reward += reward.item()\n",
    "        total_actorloss += actor_loss\n",
    "        total_criticloss += critic_loss\n",
    "        num_step = step\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return num_step, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save model weight.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    file_path_actor = os.path.join(path, filename)\n",
    "\n",
    "    # print(self.policy_net.state_dict())\n",
    "    torch.save(self.actor.state_dict(), file_path_actor)\n",
    "    print(f\"[INFO] Saved model weights to {file_path_actor}\")\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load model weight.\n",
    "    \"\"\"\n",
    "    # ========= put your code here ========= #\n",
    "    file_path_actor = os.path.join(path, filename)\n",
    "\n",
    "    if os.path.exists(file_path_actor):\n",
    "        self.actor.load_state_dict(torch.load(file_path_actor, map_location=self.device))\n",
    "        print(f\"[INFO] Loaded model weights from {file_path_actor}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"[ERROR] File not found: {file_path_actor}\")\n",
    "    # ====================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 3: Trainning & Playing to stabilize Cart-Pole Agent**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the training loop in train script and main() in the play script (in the \"Can be modified\" area of both files). Additionally, you must collect data, analyze results, and save models for evaluating agent performance.\n",
    "\n",
    "- Training the Agent\n",
    "\n",
    "    - Stabilizing Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/train.py --task SwingUp-Isaac-Cartpole-v0\n",
    "        ```\n",
    "\n",
    "- Playing\n",
    "\n",
    "    - Stabilize Cart-Pole Task\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task Stabilize-Isaac-Cartpole-v0\n",
    "        ``` \n",
    "\n",
    "    - Swing-up Cart-Pole Task (Optional)\n",
    "\n",
    "        ```python\n",
    "        python scripts/Function_based/play.py --task SwingUp-Isaac-Cartpole-v0 \n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**train.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example A2C algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_episodes = 2002\n",
    "param_grid = {\n",
    "    \"num_of_action\":[7],\n",
    "    \"action_range\":[20.0],\n",
    "    \"learning_rate\": [0.0003],\n",
    "    \"hidden_dim\": [128],\n",
    "    \"discount\":[0.99]\n",
    "}\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"device: \", device)\n",
    "\n",
    "task_name = str(args_cli.task).split('-')[0]  # Stabilize, SwingUp\n",
    "Algorithm_name = \"A2C\"\n",
    "\n",
    "# Create all combinations\n",
    "grid = list(itertools.product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "# Do Grid Search\n",
    "for config_idx, values in enumerate(grid):\n",
    "    config = dict(zip(param_names, values))\n",
    "    print(f\"\\n===== Training Config {config_idx+1}/{len(grid)}: {config} =====\")\n",
    "\n",
    "    Experiment = \"Discount \"+str(config[\"discount\"])\n",
    "\n",
    "    # Initialize Weights and Biases (wandb) for tracking and logging metrics during training\n",
    "    wandb.init( # type: ignore\n",
    "        project='DRL_HW3',  # The name of the project in wandb\n",
    "        name=\"A2C_discrete\"  # The name of the current run\n",
    "    )\n",
    "\n",
    "    # Define Agent\n",
    "    agent = A2C_Discrete(\n",
    "        device=device,\n",
    "        num_of_action=config[\"num_of_action\"],\n",
    "        action_range=[-config[\"action_range\"], config[\"action_range\"]],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        discount_factor=config[\"discount\"]\n",
    "    )\n",
    "\n",
    "    # reset environment\n",
    "    obs, _ = env.reset()\n",
    "    timestep = 0\n",
    "    # simulate environment\n",
    "    while simulation_app.is_running():\n",
    "        # run everything in inference mode\n",
    "        # with torch.inference_mode():\n",
    "        sum_reward = 0.0\n",
    "        sum_step = 0\n",
    "        for episode in tqdm(range(n_episodes)): # type: ignore  \n",
    "\n",
    "            step, reward = agent.learn(env, max_steps=1000, num_agents=1)\n",
    "\n",
    "            sum_step += step\n",
    "            sum_reward += reward\n",
    "            wandb.log({ # type: ignore\n",
    "                'num_step': step,\n",
    "                'reward': reward\n",
    "            })\n",
    "\n",
    "            if episode % 100 == 0: # type: ignore\n",
    "                print(sum_step / 100.0)\n",
    "                \n",
    "                # Show average step and reward every 100 steps\n",
    "                wandb.log({ # type: ignore\n",
    "                    'avg_step': sum_step / 100.0,\n",
    "                    'avg_reward': sum_reward / 100.0\n",
    "                })\n",
    "                sum_step = 0\n",
    "                sum_reward = 0.0                \n",
    "\n",
    "            # ================= Save Q-Learning agent Area ===================\n",
    "            w_file = f\"{Algorithm_name}_{episode}_{agent.num_of_action}_{agent.action_range[1]}_{agent.discount_factor}_{agent.lr}_{agent.hidden}.pt\" # type: ignore\n",
    "            full_path = os.path.join(f\"w/{task_name}\", Algorithm_name)\n",
    "            agent.save_w(full_path, w_file)\n",
    "\n",
    "        print('Complete')\n",
    "        if args_cli.video:\n",
    "            timestep += 1\n",
    "            # Exit the play loop after recording one video\n",
    "            if timestep == args_cli.video_length:\n",
    "                break\n",
    "\n",
    "        break\n",
    "\n",
    "    # Finish the wandb run and save the logged metrics\n",
    "    wandb.finish() # type: ignore \n",
    "# ==================================================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**play.py**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example A2C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_of_action = 7\n",
    "action_range = [-20.0, 20.0]  \n",
    "learning_rate = 0.01\n",
    "n_episodes = 50\n",
    "discount = 0.01\n",
    "entropy_coeff = 0.01\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"device: \", device)\n",
    "\n",
    "agent = A2C_Discrete(\n",
    "    device=device,\n",
    "    num_of_action=num_of_action,\n",
    "    action_range=action_range,\n",
    "    learning_rate=learning_rate,\n",
    "    discount_factor = discount,\n",
    "    entropy_coeff=entropy_coeff\n",
    ")\n",
    "\n",
    "task_name = str(args_cli.task).split('-')[0]  # Stabilize, SwingUp\n",
    "Algorithm_name = \"A2C\"  \n",
    "episode = 4900\n",
    "q_value_file = f\"{Algorithm_name}_{episode}_{agent.num_of_action}_{agent.action_range[1]}_{agent.discount_factor}_{agent.learning_rate}_{agent.entropy_coeff}.json\"\n",
    "full_path = os.path.join(f\"w/{task_name}\", Algorithm_name)\n",
    "agent.load_w(full_path, q_value_file)\n",
    "\n",
    "# reset environment\n",
    "obs, _ = env.reset()\n",
    "timestep = 0\n",
    "\n",
    "# List สำหรับเก็บ obs\n",
    "obs_list = []\n",
    "\n",
    "# simulate environment\n",
    "while simulation_app.is_running():\n",
    "    # run everything in inference mode\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        sum_step = []\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            episode_obs = []  # เก็บ obs ของแต่ละ episode\n",
    "            step = 0\n",
    "            while not done:\n",
    "                episode_obs.append([float(obs['policy'][0, i]) for i in range(4)])\n",
    "                obs = torch.tensor([obs['policy'][0, i] for i in range(4)], dtype=torch.float32).to(agent.device)\n",
    "                # agent stepping\n",
    "                action = agent.select_action(obs)\n",
    "                scaled_action = agent.scale_action(action).view(1, -1)\n",
    "\n",
    "                # env stepping\n",
    "                next_obs, reward, terminated, truncated, _ = env.step(scaled_action)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                obs = next_obs\n",
    "\n",
    "                step += 1\n",
    "\n",
    "            # print(\"Episode:\", episode+1, \" Step: \",step)\n",
    "            sum_step.append(step)\n",
    "\n",
    "            # บันทึก obs ของ episode นี้\n",
    "            obs_list.append(episode_obs)            \n",
    "\n",
    "    if args_cli.video:\n",
    "        timestep += 1\n",
    "        # Exit the play loop after recording one video\n",
    "        if timestep == args_cli.video_length:\n",
    "            break\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the position of pole of the longest duration episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save obs in JSON\n",
    "os.makedirs(\"saved_obs\", exist_ok=True)\n",
    "obs_file_path = os.path.join(\"saved_obs\", f\"{Algorithm_name}_{agent.num_of_action}_{agent.action_range[1]}_{agent.discount_factor}_{agent.learning_rate}_{agent.entropy_coeff}.json\")\n",
    "\n",
    "obs_list = [obs.detach().cpu().tolist() if isinstance(obs, torch.Tensor) else obs for obs in obs_list]\n",
    "\n",
    "with open(obs_file_path, \"w\") as f:\n",
    "    json.dump(obs_list, f, indent=4)\n",
    "\n",
    "print(f\"Observations saved to {obs_file_path}\")\n",
    "\n",
    "# choose the episode that the longest step\n",
    "max_n = np.argmax(sum_step)\n",
    "\n",
    "# choose observation of the best episode\n",
    "sublist = obs_list[max_n]  # เปลี่ยน index ตามต้องการ\n",
    "\n",
    "y_values = [item[1] for item in sublist] #*180.0/pi\n",
    "\n",
    "x_values = list(range(1, len(sublist) + 1))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_values, y_values, linestyle='-', color='b', label=\"pole_pose\")\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Pole_pose (rad)\")\n",
    "plt.title(f\"{Algorithm_name}_{agent.num_of_action}_{agent.action_range[1]}_{agent.discount_factor}_{agent.learning_rate}_{agent.entropy_coeff}\")\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "save_dir = \"plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # สร้างโฟลเดอร์ถ้ายังไม่มี\n",
    "\n",
    "save_path = os.path.join(save_dir, f\"{Algorithm_name}_{agent.num_of_action}_{agent.action_range[1]}_{agent.discount_factor}_{agent.learning_rate}_{agent.entropy_coeff}.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"📸 Saved plot as {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">**Part 4: Evaluate Cart-Pole Agent performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must evaluate the agent's performance in terms of learning efficiency (i.e., how well the agent learns to receive higher rewards) and deployment performance (i.e., how well the agent performs in the Cart-Pole problem). Analyze and visualize the results to determine:\n",
    "\n",
    "- Which algorithm performs best?\n",
    "\n",
    "- Why does it perform better than the others?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Setup**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use 5 rewards as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardsCfg:\n",
    "    \"\"\"Reward terms for the MDP.\"\"\"\n",
    "\n",
    "    # (1) Constant running reward\n",
    "    alive = RewTerm(func=mdp.is_alive, weight=1.0) # type: ignore\n",
    "    # (2) Failure penalty\n",
    "    terminating = RewTerm(func=mdp.is_terminated, weight=-2.0) # type: ignore\n",
    "    # (3) Primary task: keep pole upright\n",
    "    pole_pos = RewTerm(\n",
    "        func=mdp.joint_pos_target_l2,\n",
    "        weight=-1.0,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n",
    "    )\n",
    "    # (4) Shaping tasks: lower cart velocity\n",
    "    cart_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1, # type: ignore\n",
    "        weight=-0.01,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"])},\n",
    "    )\n",
    "    # (5) Shaping tasks: lower pole angular velocity\n",
    "    pole_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1, # type: ignore\n",
    "        weight=-0.005,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"])},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use reward per dt as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(self, dt: float) -> torch.Tensor:\n",
    "    \"\"\"Computes the reward signal as a weighted sum of individual terms.\n",
    "\n",
    "    This function calls each reward term managed by the class and adds them to compute the net\n",
    "    reward signal. It also updates the episodic sums corresponding to individual reward terms.\n",
    "\n",
    "    Args:\n",
    "        dt: The time-step interval of the environment.\n",
    "\n",
    "    Returns:\n",
    "        The net reward signal of shape (num_envs,).\n",
    "    \"\"\"\n",
    "    # reset computation\n",
    "    self._reward_buf[:] = 0.0\n",
    "    # iterate over all the reward terms\n",
    "    for name, term_cfg in zip(self._term_names, self._term_cfgs):\n",
    "        # skip if weight is zero (kind of a micro-optimization)\n",
    "        if term_cfg.weight == 0.0:\n",
    "            continue\n",
    "        # compute term's value\n",
    "        value = term_cfg.func(self._env, **term_cfg.params) * term_cfg.weight * dt\n",
    "        # update total reward\n",
    "        self._reward_buf += value\n",
    "        # update episodic sum\n",
    "        self._episode_sums[name] += value\n",
    "\n",
    "        # Update current reward for this step.\n",
    "        self._step_reward[:, self._term_names.index(name)] = value / dt\n",
    "\n",
    "    return self._reward_buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**Find the best hyperparater of each algorithm**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will find the best hyperparameter of each algorithm by do grid search of combination of hyperparameter that I interested and pick the best combination of hyperparameter by choosing the best learning curve. Example of Linear Q Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Q has 3 hyperparameter that is discount factor, learning rate, and epsilon decay.\n",
    "First, I choose 3 different number of discount factor (0.01, 0.5, and 0.99) and train. Other hyperparameter is same value for every train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"plots/LinearQ_discount.png\" alt=\"Image 1\" style=\"width: 30%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount 0.01 is the best hyperparameter, then Fix Discount Factor to 0.01 and choose different number of learning rate (0.0001, 0.001, 0.01) and fix the epsilon decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"plots/LinearQ_learning_rate.png\" alt=\"Image 1\" style=\"width: 30%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate 0.01 is the best hyperparameter, then fix Discount Factor to 0.01 and Learning rate to 0.01 and choose different number of epsilon decay (0.0003, 0.0006, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"plots/LinearQ_epsilon_decay.png\" alt=\"Image 1\" style=\"width: 30%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon decay 0.0003 is the best hyperparameter, Now, I find the best of combination of hyperparameter that is Discount Factor 0.01, Learning rate 0.01 and Epsilon decay 0.0003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do like this with another algorithm. The best hyperparameter of each algorithm is shown below.\n",
    "\n",
    "- Every Algorithm\n",
    "\n",
    "    - num_of_action = 7\n",
    "\n",
    "    - action_range = [-20.0, 20.0]  \n",
    "\n",
    "- Linear Q\n",
    "\n",
    "    - Discount Factor 0.01\n",
    "\n",
    "    - Learning Rate 0.01\n",
    "\n",
    "    - Epsilon Decay 0.0003\n",
    "\n",
    "- DQN\n",
    "\n",
    "    - Discount Factor 0.01\n",
    "\n",
    "    - Learning Rate 0.0001\n",
    "    \n",
    "    - Epsilon Decay 0.0003\n",
    "\n",
    "    - Tau 0.001\n",
    "\n",
    "    - Hidden Dim 128\n",
    "    \n",
    "    - Buffer Size 1000\n",
    "\n",
    "    - Batch Size 32\n",
    "\n",
    "- MC-REINFORCE\n",
    "\n",
    "    - Discount Factor 0.01\n",
    "\n",
    "    - Learning Rate 0.0001\n",
    "    \n",
    "    - Hidden Dim 128\n",
    "\n",
    "- A2C\n",
    "\n",
    "    - Discount Factor 0.01\n",
    "\n",
    "    - Learning Rate 0.0005\n",
    "    \n",
    "    - Hidden Dim 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of learning efficiency**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"plots/avg_step.png\" alt=\"Image 1\" style=\"width: 30%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC-REINFORCE is best algorithm because\n",
    "\n",
    "- MC-REINFORCE uses Full Return, not Value Function, so there is no bias from value estimation, resulting in clear and direct learning signal from real reward.\n",
    "\n",
    "- A2C uses TD to evaluate advantage if V(s) has not converged (critic does not learn well), which causes the advantage to be wrong and causes the actor to update in the wrong direction.\n",
    "\n",
    "- DQN has overestimation bias from max, so the gradient will flow only at argmax action. If the network does not learn well enough, it will output only bad actions.\n",
    "\n",
    "- Linear Q may still be slow to select the best action. Even a small adjustment of weights will make it learn poorly, so it may need to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">**In term of deployment performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 0px;\">\n",
    "    <img src=\"plots/LinearQ_7_20.0_0.01_0.01_0.0003.png\" alt=\"Image 1\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/DQN_7_20.0_0.01_0.0001_0.0003_128_1000_32_0.001.png\" alt=\"Image 2\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/MC_REINFORCE_7_20.0_0.01_0.001_128.png\" alt=\"Image 3\" style=\"width: 100%; height: auto;\">\n",
    "    <img src=\"plots/A2C_7_20.0_0.99_0.0005_128.png\" alt=\"Image 4\" style=\"width: 100%; height: auto;\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best algorithm is Linear Q because\n",
    "\n",
    "- The resulting Linear Q policy is relatively straightforward, so it can select actions that maintain balance, allowing the pole to be controlled within a narrow range for a long time.\n",
    "\n",
    "- The MC-REINFORCE Policy is more random than the Gradient from full return, with variance, making the final policy still quite random, causing the pole to still oscillate.\n",
    "\n",
    "- For DQN, Policy from Q-network may overfit with inconsistent Q-value, thus making argmax action selection from Q is wrong, causing the pole to slowly tilt down and fall.\n",
    "\n",
    "- A2C with Actor network is driven by advantage from critic, if critic still has bias, the resulting policy is not good enough, so the pole swings less than DQN/REINFORCE but still falls down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
