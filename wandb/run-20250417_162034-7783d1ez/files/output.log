===== Training Config 1/1: {'num_of_action': 7, 'action_range': 20.0, 'learning_rate': 0.001, 'epsilon_decay': 0.0003, 'discount': 0.01} =====
State:  tensor([[-0.2300,  0.0700, -0.1187, -0.1686]], device='cuda:0')
action:  2
scale action:  tensor([[-6.6667]])
  0%|                                                                                                                                                                              | 0/5000 [00:00<?, ?it/s]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/talae/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/utils/hydra.py", line 101, in hydra_main
    func(env_cfg, agent_cfg, *args, **kwargs)
  File "/home/talae/DRL_HW3/DRL_HW3/scripts/Function_based/train_linearQ.py", line 173, in main
    step, reward = agent.learn(env, 1000)
  File "/home/talae/DRL_HW3/DRL_HW3/RL_Algorithm/Function_based/Linear_Q.py", line 164, in learn
    self.update(obs_list, action, reward, next_obs_list, done)
  File "/home/talae/DRL_HW3/DRL_HW3/RL_Algorithm/Function_based/Linear_Q.py", line 83, in update
    current_q = self.q(phi_s, a=action)
  File "/home/talae/DRL_HW3/DRL_HW3/RL_Algorithm/Function_based/Linear_Q.py", line 127, in q
    return q_values[a]
IndexError: index 2 is out of bounds for axis 0 with size 1
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.